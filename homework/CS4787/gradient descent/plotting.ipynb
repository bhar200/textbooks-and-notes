{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Measurement 1: 0.09621666666666662\n",
      "Accuracy Measurement 2: 0.5865\n",
      "Accuracy Measurement 3: 0.7120166666666666\n",
      "Accuracy Measurement 4: 0.7595333333333333\n",
      "Accuracy Measurement 5: 0.7866\n",
      "Accuracy Measurement 6: 0.804\n",
      "Accuracy Measurement 7: 0.8170666666666667\n",
      "Accuracy Measurement 8: 0.8265166666666667\n",
      "Accuracy Measurement 9: 0.8337833333333333\n",
      "Accuracy Measurement 10: 0.8396\n",
      "Accuracy Measurement 11: 0.8443666666666667\n",
      "Accuracy Measurement 12: 0.8489333333333333\n",
      "Accuracy Measurement 13: 0.8521166666666666\n",
      "Accuracy Measurement 14: 0.8555833333333334\n",
      "Accuracy Measurement 15: 0.8588166666666667\n",
      "Accuracy Measurement 16: 0.86165\n",
      "Accuracy Measurement 17: 0.8639\n",
      "Accuracy Measurement 18: 0.86595\n",
      "Accuracy Measurement 19: 0.86795\n",
      "Accuracy Measurement 20: 0.8698166666666667\n",
      "Accuracy Measurement 21: 0.8714166666666666\n",
      "Accuracy Measurement 22: 0.8728333333333333\n",
      "Accuracy Measurement 23: 0.8744666666666666\n",
      "Accuracy Measurement 24: 0.87575\n",
      "Accuracy Measurement 25: 0.8773833333333333\n",
      "Accuracy Measurement 26: 0.8786333333333334\n",
      "Accuracy Measurement 27: 0.8799166666666667\n",
      "Accuracy Measurement 28: 0.881\n",
      "Accuracy Measurement 29: 0.8818666666666667\n",
      "Accuracy Measurement 30: 0.883\n",
      "Accuracy Measurement 31: 0.8841833333333333\n",
      "Accuracy Measurement 32: 0.8851833333333333\n",
      "Accuracy Measurement 33: 0.8861\n",
      "Accuracy Measurement 34: 0.8869\n",
      "Accuracy Measurement 35: 0.8878333333333334\n",
      "Accuracy Measurement 36: 0.8884166666666666\n",
      "Accuracy Measurement 37: 0.8891\n",
      "Accuracy Measurement 38: 0.8897666666666667\n",
      "Accuracy Measurement 39: 0.8904833333333333\n",
      "Accuracy Measurement 40: 0.8911333333333333\n",
      "Accuracy Measurement 41: 0.8917333333333334\n",
      "Accuracy Measurement 42: 0.8923\n",
      "Accuracy Measurement 43: 0.8929833333333334\n",
      "Accuracy Measurement 44: 0.8936166666666666\n",
      "Accuracy Measurement 45: 0.8942166666666667\n",
      "Accuracy Measurement 46: 0.8947166666666667\n",
      "Accuracy Measurement 47: 0.8951333333333333\n",
      "Accuracy Measurement 48: 0.8956166666666666\n",
      "Accuracy Measurement 49: 0.8963\n",
      "Accuracy Measurement 50: 0.8969666666666667\n",
      "Accuracy Measurement 51: 0.8974666666666666\n",
      "Accuracy Measurement 52: 0.8978166666666667\n",
      "Accuracy Measurement 53: 0.8982666666666667\n",
      "Accuracy Measurement 54: 0.8986666666666666\n",
      "Accuracy Measurement 55: 0.8990166666666667\n",
      "Accuracy Measurement 56: 0.8994333333333333\n",
      "Accuracy Measurement 57: 0.8997166666666667\n",
      "Accuracy Measurement 58: 0.9001833333333333\n",
      "Accuracy Measurement 59: 0.90045\n",
      "Accuracy Measurement 60: 0.9009\n",
      "Accuracy Measurement 61: 0.9012833333333333\n",
      "Accuracy Measurement 62: 0.9019333333333334\n",
      "Accuracy Measurement 63: 0.90235\n",
      "Accuracy Measurement 64: 0.9026666666666666\n",
      "Accuracy Measurement 65: 0.90305\n",
      "Accuracy Measurement 66: 0.9033833333333333\n",
      "Accuracy Measurement 67: 0.9036666666666666\n",
      "Accuracy Measurement 68: 0.9040666666666667\n",
      "Accuracy Measurement 69: 0.9042833333333333\n",
      "Accuracy Measurement 70: 0.9044833333333333\n",
      "Accuracy Measurement 71: 0.90475\n",
      "Accuracy Measurement 72: 0.9051833333333333\n",
      "Accuracy Measurement 73: 0.90525\n",
      "Accuracy Measurement 74: 0.90535\n",
      "Accuracy Measurement 75: 0.9055666666666666\n",
      "Accuracy Measurement 76: 0.906\n",
      "Accuracy Measurement 77: 0.9064333333333333\n",
      "Accuracy Measurement 78: 0.9067\n",
      "Accuracy Measurement 79: 0.907\n",
      "Accuracy Measurement 80: 0.9072833333333333\n",
      "Accuracy Measurement 81: 0.9074166666666666\n",
      "Accuracy Measurement 82: 0.90765\n",
      "Accuracy Measurement 83: 0.90785\n",
      "Accuracy Measurement 84: 0.9079833333333334\n",
      "Accuracy Measurement 85: 0.9080333333333334\n",
      "Accuracy Measurement 86: 0.9081666666666667\n",
      "Accuracy Measurement 87: 0.9083\n",
      "Accuracy Measurement 88: 0.9084666666666666\n",
      "Accuracy Measurement 89: 0.90865\n",
      "Accuracy Measurement 90: 0.9088333333333334\n",
      "Accuracy Measurement 91: 0.9089833333333334\n",
      "Accuracy Measurement 92: 0.90925\n",
      "Accuracy Measurement 93: 0.90945\n",
      "Accuracy Measurement 94: 0.90975\n",
      "Accuracy Measurement 95: 0.9099833333333334\n",
      "Accuracy Measurement 96: 0.91015\n",
      "Accuracy Measurement 97: 0.9104333333333333\n",
      "Accuracy Measurement 98: 0.9105666666666666\n",
      "Accuracy Measurement 99: 0.9108833333333334\n",
      "Accuracy Measurement 100: 0.9109166666666667\n",
      "Accuracy Measurement 101: 0.9111166666666667\n",
      "Gradient Descent (1000 Iters) Took 102.44799327850342 Seconds\n",
      "Accuracy Measurement 1: 0.11339999999999995\n",
      "Accuracy Measurement 2: 0.7116666666666667\n",
      "Accuracy Measurement 3: 0.7846833333333333\n",
      "Accuracy Measurement 4: 0.81805\n",
      "Accuracy Measurement 5: 0.8363\n",
      "Accuracy Measurement 6: 0.8462\n",
      "Accuracy Measurement 7: 0.8551166666666666\n",
      "Accuracy Measurement 8: 0.8612\n",
      "Accuracy Measurement 9: 0.8671666666666666\n",
      "Accuracy Measurement 10: 0.8722833333333333\n",
      "Accuracy Measurement 11: 0.8748666666666667\n",
      "Accuracy Measurement 12: 0.8758666666666667\n",
      "Accuracy Measurement 13: 0.8819\n",
      "Accuracy Measurement 14: 0.8845166666666666\n",
      "Accuracy Measurement 15: 0.8857166666666667\n",
      "Accuracy Measurement 16: 0.88715\n",
      "Accuracy Measurement 17: 0.8852666666666666\n",
      "Accuracy Measurement 18: 0.8909\n",
      "Accuracy Measurement 19: 0.89055\n",
      "Accuracy Measurement 20: 0.8929833333333334\n",
      "Accuracy Measurement 21: 0.892\n",
      "Accuracy Measurement 22: 0.8930333333333333\n",
      "Accuracy Measurement 23: 0.8965666666666666\n",
      "Accuracy Measurement 24: 0.8974833333333333\n",
      "Accuracy Measurement 25: 0.8974833333333333\n",
      "Accuracy Measurement 26: 0.89935\n",
      "Accuracy Measurement 27: 0.9000666666666667\n",
      "Accuracy Measurement 28: 0.9006666666666666\n",
      "Accuracy Measurement 29: 0.9007166666666667\n",
      "Accuracy Measurement 30: 0.9004\n",
      "Accuracy Measurement 31: 0.9029333333333334\n",
      "Accuracy Measurement 32: 0.9015166666666666\n",
      "Accuracy Measurement 33: 0.9039833333333334\n",
      "Accuracy Measurement 34: 0.9044166666666666\n",
      "Accuracy Measurement 35: 0.9048\n",
      "Accuracy Measurement 36: 0.9059\n",
      "Accuracy Measurement 37: 0.9059166666666667\n",
      "Accuracy Measurement 38: 0.9065166666666666\n",
      "Accuracy Measurement 39: 0.9054\n",
      "Accuracy Measurement 40: 0.9054166666666666\n",
      "Accuracy Measurement 41: 0.9077666666666667\n",
      "Accuracy Measurement 42: 0.9062166666666667\n",
      "Accuracy Measurement 43: 0.9085666666666666\n",
      "Accuracy Measurement 44: 0.90945\n",
      "Accuracy Measurement 45: 0.9097\n",
      "Accuracy Measurement 46: 0.90995\n",
      "Accuracy Measurement 47: 0.9107333333333333\n",
      "Accuracy Measurement 48: 0.90975\n",
      "Accuracy Measurement 49: 0.905\n",
      "Accuracy Measurement 50: 0.9110333333333334\n",
      "Accuracy Measurement 51: 0.90945\n",
      "Accuracy Measurement 52: 0.9103\n",
      "Accuracy Measurement 53: 0.9090166666666667\n",
      "Accuracy Measurement 54: 0.91145\n",
      "Accuracy Measurement 55: 0.9102833333333333\n",
      "Accuracy Measurement 56: 0.91285\n",
      "Accuracy Measurement 57: 0.9139333333333334\n",
      "Accuracy Measurement 58: 0.9125833333333333\n",
      "Accuracy Measurement 59: 0.91165\n",
      "Accuracy Measurement 60: 0.9128\n",
      "Accuracy Measurement 61: 0.9126\n",
      "Accuracy Measurement 62: 0.9128833333333334\n",
      "Accuracy Measurement 63: 0.9144166666666667\n",
      "Accuracy Measurement 64: 0.9147666666666666\n",
      "Accuracy Measurement 65: 0.9131666666666667\n",
      "Accuracy Measurement 66: 0.91435\n",
      "Accuracy Measurement 67: 0.9149833333333334\n",
      "Accuracy Measurement 68: 0.9149166666666667\n",
      "Accuracy Measurement 69: 0.9177333333333333\n",
      "Accuracy Measurement 70: 0.91645\n",
      "Accuracy Measurement 71: 0.9154166666666667\n",
      "Accuracy Measurement 72: 0.9167666666666666\n",
      "Accuracy Measurement 73: 0.9167666666666666\n",
      "Accuracy Measurement 74: 0.9162166666666667\n",
      "Accuracy Measurement 75: 0.9168333333333333\n",
      "Accuracy Measurement 76: 0.9143\n",
      "Accuracy Measurement 77: 0.9168166666666666\n",
      "Accuracy Measurement 78: 0.9176666666666666\n",
      "Accuracy Measurement 79: 0.9167666666666666\n",
      "Accuracy Measurement 80: 0.91925\n",
      "Accuracy Measurement 81: 0.9179333333333334\n",
      "Accuracy Measurement 82: 0.9187833333333333\n",
      "Accuracy Measurement 83: 0.9179333333333334\n",
      "Accuracy Measurement 84: 0.9178\n",
      "Accuracy Measurement 85: 0.9187833333333333\n",
      "Accuracy Measurement 86: 0.9185\n",
      "Accuracy Measurement 87: 0.9179666666666667\n",
      "Accuracy Measurement 88: 0.9180666666666667\n",
      "Accuracy Measurement 89: 0.919\n",
      "Accuracy Measurement 90: 0.9177166666666666\n",
      "Accuracy Measurement 91: 0.9168666666666667\n",
      "Accuracy Measurement 92: 0.9192666666666667\n",
      "Accuracy Measurement 93: 0.9196333333333333\n",
      "Accuracy Measurement 94: 0.9174666666666667\n",
      "Accuracy Measurement 95: 0.9179166666666667\n",
      "Accuracy Measurement 96: 0.9207833333333333\n",
      "Accuracy Measurement 97: 0.91955\n",
      "Accuracy Measurement 98: 0.9201\n",
      "Accuracy Measurement 99: 0.92015\n",
      "Accuracy Measurement 100: 0.9216166666666666\n",
      "Accuracy Measurement 101: 0.9209666666666667\n",
      "SGD Normal B=25, A=0.1, gamma=0.0001 Took 3.8175151348114014 Seconds\n",
      "\n",
      "Accuracy Measurement 1: 0.11470000000000002\n",
      "Accuracy Measurement 2: 0.71135\n",
      "Accuracy Measurement 3: 0.7747166666666667\n",
      "Accuracy Measurement 4: 0.81625\n",
      "Accuracy Measurement 5: 0.8274\n",
      "Accuracy Measurement 6: 0.8427\n",
      "Accuracy Measurement 7: 0.8533166666666666\n",
      "Accuracy Measurement 8: 0.8555833333333334\n",
      "Accuracy Measurement 9: 0.8595333333333334\n",
      "Accuracy Measurement 10: 0.86815\n",
      "Accuracy Measurement 11: 0.87085\n",
      "Accuracy Measurement 12: 0.8767166666666667\n",
      "Accuracy Measurement 13: 0.8751333333333333\n",
      "Accuracy Measurement 14: 0.8817833333333334\n",
      "Accuracy Measurement 15: 0.8811166666666667\n",
      "Accuracy Measurement 16: 0.8848833333333334\n",
      "Accuracy Measurement 17: 0.8862666666666666\n",
      "Accuracy Measurement 18: 0.8841833333333333\n",
      "Accuracy Measurement 19: 0.8862333333333333\n",
      "Accuracy Measurement 20: 0.8893\n",
      "Accuracy Measurement 21: 0.88965\n",
      "Accuracy Measurement 22: 0.8945333333333333\n",
      "Accuracy Measurement 23: 0.8910333333333333\n",
      "Accuracy Measurement 24: 0.8957\n",
      "Accuracy Measurement 25: 0.89705\n",
      "Accuracy Measurement 26: 0.8975833333333333\n",
      "Accuracy Measurement 27: 0.8989\n",
      "Accuracy Measurement 28: 0.89565\n",
      "Accuracy Measurement 29: 0.89715\n",
      "Accuracy Measurement 30: 0.8989166666666667\n",
      "Accuracy Measurement 31: 0.8982333333333333\n",
      "Accuracy Measurement 32: 0.9029\n",
      "Accuracy Measurement 33: 0.8995\n",
      "Accuracy Measurement 34: 0.90305\n",
      "Accuracy Measurement 35: 0.9039333333333334\n",
      "Accuracy Measurement 36: 0.9050833333333334\n",
      "Accuracy Measurement 37: 0.9053666666666667\n",
      "Accuracy Measurement 38: 0.9019333333333334\n",
      "Accuracy Measurement 39: 0.9032\n",
      "Accuracy Measurement 40: 0.9045166666666666\n",
      "Accuracy Measurement 41: 0.9021666666666667\n",
      "Accuracy Measurement 42: 0.9089333333333334\n",
      "Accuracy Measurement 43: 0.9045166666666666\n",
      "Accuracy Measurement 44: 0.90795\n",
      "Accuracy Measurement 45: 0.909\n",
      "Accuracy Measurement 46: 0.9093833333333333\n",
      "Accuracy Measurement 47: 0.9096666666666666\n",
      "Accuracy Measurement 48: 0.90565\n",
      "Accuracy Measurement 49: 0.9077166666666666\n",
      "Accuracy Measurement 50: 0.9087166666666666\n",
      "Accuracy Measurement 51: 0.9061\n",
      "Accuracy Measurement 52: 0.9123166666666667\n",
      "Accuracy Measurement 53: 0.9088333333333334\n",
      "Accuracy Measurement 54: 0.9112166666666667\n",
      "Accuracy Measurement 55: 0.9128833333333334\n",
      "Accuracy Measurement 56: 0.9122\n",
      "Accuracy Measurement 57: 0.9126333333333333\n",
      "Accuracy Measurement 58: 0.9087666666666666\n",
      "Accuracy Measurement 59: 0.91055\n",
      "Accuracy Measurement 60: 0.9119166666666667\n",
      "Accuracy Measurement 61: 0.9088\n",
      "Accuracy Measurement 62: 0.9152\n",
      "Accuracy Measurement 63: 0.9115666666666666\n",
      "Accuracy Measurement 64: 0.9137\n",
      "Accuracy Measurement 65: 0.9153333333333333\n",
      "Accuracy Measurement 66: 0.9145833333333333\n",
      "Accuracy Measurement 67: 0.9151166666666667\n",
      "Accuracy Measurement 68: 0.9110333333333334\n",
      "Accuracy Measurement 69: 0.91305\n",
      "Accuracy Measurement 70: 0.9141333333333334\n",
      "Accuracy Measurement 71: 0.911\n",
      "Accuracy Measurement 72: 0.9168833333333334\n",
      "Accuracy Measurement 73: 0.9137166666666666\n",
      "Accuracy Measurement 74: 0.9155666666666666\n",
      "Accuracy Measurement 75: 0.9177833333333333\n",
      "Accuracy Measurement 76: 0.9166166666666666\n",
      "Accuracy Measurement 77: 0.9167333333333333\n",
      "Accuracy Measurement 78: 0.9131333333333334\n",
      "Accuracy Measurement 79: 0.9149666666666667\n",
      "Accuracy Measurement 80: 0.9155833333333333\n",
      "Accuracy Measurement 81: 0.9127\n",
      "Accuracy Measurement 82: 0.9182833333333333\n",
      "Accuracy Measurement 83: 0.9156\n",
      "Accuracy Measurement 84: 0.9172833333333333\n",
      "Accuracy Measurement 85: 0.9194666666666667\n",
      "Accuracy Measurement 86: 0.9180833333333334\n",
      "Accuracy Measurement 87: 0.91855\n",
      "Accuracy Measurement 88: 0.9150333333333334\n",
      "Accuracy Measurement 89: 0.91665\n",
      "Accuracy Measurement 90: 0.9168833333333334\n",
      "Accuracy Measurement 91: 0.9138333333333334\n",
      "Accuracy Measurement 92: 0.9199333333333334\n",
      "Accuracy Measurement 93: 0.9168\n",
      "Accuracy Measurement 94: 0.9187666666666666\n",
      "Accuracy Measurement 95: 0.9207\n",
      "Accuracy Measurement 96: 0.9193166666666667\n",
      "Accuracy Measurement 97: 0.9196333333333333\n",
      "Accuracy Measurement 98: 0.9164166666666667\n",
      "Accuracy Measurement 99: 0.91785\n",
      "Accuracy Measurement 100: 0.9185333333333333\n",
      "Accuracy Measurement 101: 0.9152833333333333\n",
      "SGD Sequential B=25, A=0.1, gamma=0.0001 Took 2.2371199131011963 Seconds\n",
      "\n",
      "Accuracy Measurement 1: 0.08256666666666668\n",
      "Accuracy Measurement 2: 0.7041333333333334\n",
      "Accuracy Measurement 3: 0.7873333333333333\n",
      "Accuracy Measurement 4: 0.8217166666666667\n",
      "Accuracy Measurement 5: 0.8326\n",
      "Accuracy Measurement 6: 0.8472833333333334\n",
      "Accuracy Measurement 7: 0.8552833333333333\n",
      "Accuracy Measurement 8: 0.8591666666666666\n",
      "Accuracy Measurement 9: 0.86595\n",
      "Accuracy Measurement 10: 0.8710166666666667\n",
      "Accuracy Measurement 11: 0.8761\n",
      "Accuracy Measurement 12: 0.8780666666666667\n",
      "Accuracy Measurement 13: 0.8804666666666666\n",
      "Accuracy Measurement 14: 0.8825\n",
      "Accuracy Measurement 15: 0.8859333333333334\n",
      "Accuracy Measurement 16: 0.88525\n",
      "Accuracy Measurement 17: 0.8890666666666667\n",
      "Accuracy Measurement 18: 0.89025\n",
      "Accuracy Measurement 19: 0.8930666666666667\n",
      "Accuracy Measurement 20: 0.8923833333333333\n",
      "Accuracy Measurement 21: 0.89115\n",
      "Accuracy Measurement 22: 0.89475\n",
      "Accuracy Measurement 23: 0.8954\n",
      "Accuracy Measurement 24: 0.8956666666666667\n",
      "Accuracy Measurement 25: 0.8995\n",
      "Accuracy Measurement 26: 0.8975833333333333\n",
      "Accuracy Measurement 27: 0.8980666666666667\n",
      "Accuracy Measurement 28: 0.9003166666666667\n",
      "Accuracy Measurement 29: 0.898\n",
      "Accuracy Measurement 30: 0.8995333333333333\n",
      "Accuracy Measurement 31: 0.8987166666666667\n",
      "Accuracy Measurement 32: 0.9007666666666667\n",
      "Accuracy Measurement 33: 0.9039166666666667\n",
      "Accuracy Measurement 34: 0.9006166666666666\n",
      "Accuracy Measurement 35: 0.9035\n",
      "Accuracy Measurement 36: 0.90515\n",
      "Accuracy Measurement 37: 0.9013333333333333\n",
      "Accuracy Measurement 38: 0.9057333333333333\n",
      "Accuracy Measurement 39: 0.9066333333333333\n",
      "Accuracy Measurement 40: 0.9054666666666666\n",
      "Accuracy Measurement 41: 0.9059\n",
      "Accuracy Measurement 42: 0.9084666666666666\n",
      "Accuracy Measurement 43: 0.90705\n",
      "Accuracy Measurement 44: 0.9099833333333334\n",
      "Accuracy Measurement 45: 0.90965\n",
      "Accuracy Measurement 46: 0.9058166666666667\n",
      "Accuracy Measurement 47: 0.9106666666666666\n",
      "Accuracy Measurement 48: 0.90985\n",
      "Accuracy Measurement 49: 0.9088833333333334\n",
      "Accuracy Measurement 50: 0.9115166666666666\n",
      "Accuracy Measurement 51: 0.9131166666666667\n",
      "Accuracy Measurement 52: 0.9114333333333333\n",
      "Accuracy Measurement 53: 0.9113333333333333\n",
      "Accuracy Measurement 54: 0.9124666666666666\n",
      "Accuracy Measurement 55: 0.9125166666666666\n",
      "Accuracy Measurement 56: 0.9123333333333333\n",
      "Accuracy Measurement 57: 0.9147\n",
      "Accuracy Measurement 58: 0.9136833333333333\n",
      "Accuracy Measurement 59: 0.9146666666666666\n",
      "Accuracy Measurement 60: 0.9147666666666666\n",
      "Accuracy Measurement 61: 0.9152\n",
      "Accuracy Measurement 62: 0.9170833333333334\n",
      "Accuracy Measurement 63: 0.9146666666666666\n",
      "Accuracy Measurement 64: 0.915\n",
      "Accuracy Measurement 65: 0.91715\n",
      "Accuracy Measurement 66: 0.91395\n",
      "Accuracy Measurement 67: 0.9139666666666667\n",
      "Accuracy Measurement 68: 0.9156833333333333\n",
      "Accuracy Measurement 69: 0.91635\n",
      "Accuracy Measurement 70: 0.9175\n",
      "Accuracy Measurement 71: 0.9170833333333334\n",
      "Accuracy Measurement 72: 0.9166166666666666\n",
      "Accuracy Measurement 73: 0.9171\n",
      "Accuracy Measurement 74: 0.9165166666666666\n",
      "Accuracy Measurement 75: 0.91625\n",
      "Accuracy Measurement 76: 0.9181666666666667\n",
      "Accuracy Measurement 77: 0.918\n",
      "Accuracy Measurement 78: 0.9177\n",
      "Accuracy Measurement 79: 0.9182\n",
      "Accuracy Measurement 80: 0.9155833333333333\n",
      "Accuracy Measurement 81: 0.9202666666666667\n",
      "Accuracy Measurement 82: 0.9186833333333333\n",
      "Accuracy Measurement 83: 0.9187833333333333\n",
      "Accuracy Measurement 84: 0.9184\n",
      "Accuracy Measurement 85: 0.9188166666666666\n",
      "Accuracy Measurement 86: 0.9192333333333333\n",
      "Accuracy Measurement 87: 0.9192166666666667\n",
      "Accuracy Measurement 88: 0.9205666666666666\n",
      "Accuracy Measurement 89: 0.9176333333333333\n",
      "Accuracy Measurement 90: 0.9165166666666666\n",
      "Accuracy Measurement 91: 0.92045\n",
      "Accuracy Measurement 92: 0.9191166666666667\n",
      "Accuracy Measurement 93: 0.9199833333333334\n",
      "Accuracy Measurement 94: 0.9177\n",
      "Accuracy Measurement 95: 0.9185666666666666\n",
      "Accuracy Measurement 96: 0.9170166666666667\n",
      "Accuracy Measurement 97: 0.91865\n",
      "Accuracy Measurement 98: 0.9210666666666667\n",
      "Accuracy Measurement 99: 0.9210833333333334\n",
      "Accuracy Measurement 100: 0.9202\n",
      "Accuracy Measurement 101: 0.9210166666666667\n",
      "SGD Random Shuffling B=25, A=0.1, gamma=0.0001 Took 3.68811297416687 Seconds\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy\n",
    "from numpy import random\n",
    "import scipy\n",
    "from scipy.special import softmax\n",
    "import mnist\n",
    "import pickle\n",
    "\n",
    "# you can use matplotlib for plotting\n",
    "from time import time\n",
    "import matplotlib\n",
    "from matplotlib import pyplot\n",
    "\n",
    "mnist_data_directory = os.path.join(os.path.abspath(''), \"data\")\n",
    "\n",
    "# TODO add any additional imports and global variables\n",
    "\n",
    "\n",
    "def load_MNIST_dataset():\n",
    "    PICKLE_FILE = os.path.join(mnist_data_directory, \"MNIST.pickle\")\n",
    "    try:\n",
    "        dataset = pickle.load(open(PICKLE_FILE, \"rb\"))\n",
    "    except:\n",
    "        # load the MNIST dataset\n",
    "        mnist_data = mnist.MNIST(mnist_data_directory, return_type=\"numpy\", gz=True)\n",
    "        Xs_tr, Lbls_tr = mnist_data.load_training()\n",
    "        Xs_tr = Xs_tr.transpose() / 255.0\n",
    "        Ys_tr = numpy.zeros((10, 60000))\n",
    "        for i in range(60000):\n",
    "            Ys_tr[Lbls_tr[i], i] = 1.0  # one-hot encode each label\n",
    "        Xs_tr = numpy.ascontiguousarray(Xs_tr)\n",
    "        Ys_tr = numpy.ascontiguousarray(Ys_tr)\n",
    "        Xs_te, Lbls_te = mnist_data.load_testing()\n",
    "        Xs_te = Xs_te.transpose() / 255.0\n",
    "        Ys_te = numpy.zeros((10, 10000))\n",
    "        for i in range(10000):\n",
    "            Ys_te[Lbls_te[i], i] = 1.0  # one-hot encode each label\n",
    "        Xs_te = numpy.ascontiguousarray(Xs_te)\n",
    "        Ys_te = numpy.ascontiguousarray(Ys_te)\n",
    "        dataset = (Xs_tr, Ys_tr, Xs_te, Ys_te)\n",
    "        pickle.dump(dataset, open(PICKLE_FILE, \"wb\"))\n",
    "    return dataset\n",
    "\n",
    "\n",
    "# compute the cross-entropy loss of the classifier\n",
    "#\n",
    "# x         examples          (d)\n",
    "# y         labels            (c)\n",
    "# gamma     L2 regularization constant\n",
    "# W         parameters        (c * d)\n",
    "#\n",
    "# returns   the model cross-entropy loss\n",
    "def multinomial_logreg_loss_i(x, y, gamma, W):\n",
    "    return numpy.dot(y, -numpy.log(softmax(W @ x)))\n",
    "\n",
    "\n",
    "# compute the gradient of a single example of the multinomial logistic regression objective, with regularization\n",
    "#\n",
    "# x         training example   (d)\n",
    "# y         training label     (c)\n",
    "# gamma     L2 regularization constant\n",
    "# W         parameters        (c * d)\n",
    "#\n",
    "# returns   the gradient of the loss with respect to the model parameters W\n",
    "def multinomial_logreg_grad_i(x, y, gamma, W):\n",
    "    return numpy.outer(softmax(W @ x) - y, x) + gamma * W\n",
    "\n",
    "\n",
    "# compute the error of the classifier\n",
    "#\n",
    "# Xs        examples          (d * n)\n",
    "# Ys        labels            (c * n)\n",
    "# W         parameters        (c * d)\n",
    "#\n",
    "# returns   the model error as a percentage of incorrect labels\n",
    "def multinomial_logreg_error(Xs, Ys, W):\n",
    "    val = softmax(W @ Xs)\n",
    "    preds = numpy.zeros_like(val)\n",
    "    preds[numpy.argmax(val, axis=0), range(val.shape[1])] = 1\n",
    "    return 1 - numpy.sum(preds * Ys) / Ys.shape[1]\n",
    "\n",
    "\n",
    "# compute the gradient of the multinomial logistic regression objective on a batch, with regularization\n",
    "#\n",
    "# Xs        training examples (d * n)\n",
    "# Ys        training labels   (c * n)\n",
    "# gamma     L2 regularization constant\n",
    "# W         parameters        (c * d)\n",
    "# ii        indices of the batch (an iterable or range)\n",
    "#\n",
    "# returns   the gradient of the model parameters\n",
    "def multinomial_logreg_batch_grad(Xs, Ys, gamma, W, ii=None):\n",
    "    if ii is None:\n",
    "        ii = range(Xs.shape[1])\n",
    "        X = Xs\n",
    "        Y = Ys\n",
    "    else:\n",
    "        X = Xs[:, ii]\n",
    "        Y = Ys[:, ii]\n",
    "    # a starter solution using an average of the example gradients\n",
    "    (d, n) = Xs.shape\n",
    "    return ((softmax(W @ X, axis=0) - Y) @ X.T) / len(ii) + gamma * W\n",
    "\n",
    "\n",
    "# compute the cross-entropy loss of the classifier on a batch, with regularization\n",
    "#\n",
    "# Xs        examples          (d * n)\n",
    "# Ys        labels            (c * n)\n",
    "# gamma     L2 regularization constant\n",
    "# W         parameters        (c * d)\n",
    "# ii        indices of the batch (an iterable or range)\n",
    "#\n",
    "# returns   the model cross-entropy loss\n",
    "def multinomial_logreg_batch_loss(Xs, Ys, gamma, W, ii=None):\n",
    "    if ii is None:\n",
    "        ii = range(Xs.shape[1])\n",
    "        X = Xs\n",
    "        Y = Ys\n",
    "    else:\n",
    "        X = Xs[:, ii]\n",
    "        Y = Ys[:, ii]\n",
    "\n",
    "    (d, n) = Xs.shape\n",
    "    frobNorm = numpy.linalg.norm(W) ** 2\n",
    "    return (\n",
    "        numpy.sum(Y * (-numpy.log(softmax(W @ X, axis=0)))) / len(ii)\n",
    "        + gamma * frobNorm / 2\n",
    "    )\n",
    "\n",
    "    # acc = 0.0\n",
    "    # for i in ii:\n",
    "    #     acc += multinomial_logreg_loss_i(Xs[:, i], Ys[:, i], gamma, W)\n",
    "    # return acc / len(ii)\n",
    "\n",
    "\n",
    "# run gradient descent on a multinomial logistic regression objective, with regularization\n",
    "#\n",
    "# Xs            training examples (d * n)\n",
    "# Ys            training labels   (d * c)\n",
    "# gamma         L2 regularization constant\n",
    "# W0            the initial value of the parameters (c * d)\n",
    "# alpha         step size/learning rate\n",
    "# num_iters     number of iterations to run\n",
    "# monitor_freq  how frequently to output the parameter vector\n",
    "#\n",
    "# returns       a list of models parameters, one every \"monitor_freq\" iterations\n",
    "#               should return model parameters before iteration 0, iteration monitor_freq, iteration 2*monitor_freq, and again at the end\n",
    "#               for a total of (num_iters/monitor_freq)+1 models, if num_iters is divisible by monitor_freq.\n",
    "def gradient_descent(Xs, Ys, gamma, W0, alpha, num_iters, monitor_freq):\n",
    "    W = W0\n",
    "    paramsHist = [W.copy()]\n",
    "    for iter in range(num_iters):\n",
    "        W = W - alpha * multinomial_logreg_batch_grad(Xs, Ys, gamma, W, None)\n",
    "        if (iter + 1) % monitor_freq == 0:\n",
    "            paramsHist.append(W.copy())\n",
    "    return paramsHist\n",
    "\n",
    "\n",
    "# ALGORITHM 1: run stochastic gradient descent on a multinomial logistic regression objective, with regularization\n",
    "#\n",
    "# Xs              training examples (d * n)\n",
    "# Ys              training labels   (c * n)\n",
    "# gamma           L2 regularization constant\n",
    "# W0              the initial value of the parameters (c * d)\n",
    "# alpha           step size/learning rate\n",
    "# B               minibatch size\n",
    "# num_epochs      number of epochs (passes through the training set) to run\n",
    "# monitor_period  how frequently, in terms of batches (not epochs) to output the parameter vector\n",
    "#\n",
    "# returns         a list of model parameters vectors, one every \"monitor_period\" batches\n",
    "#                   to do this, you'll want code like the following:\n",
    "#                     models = []\n",
    "#                     models.append(W0.copy())   # (you may not need the copy if you don't mutate W0)\n",
    "#                     ...\n",
    "#                     for sgd_iteration in ... :\n",
    "#                       ...\n",
    "#                       # code to compute a single SGD update step here\n",
    "#                       ...\n",
    "#                       if (it % monitor_period == 0):\n",
    "#                         models.append(W)\n",
    "def sgd_minibatch(Xs, Ys, gamma, W0, alpha, B, num_epochs, monitor_period):\n",
    "    W = W0\n",
    "    paramsHist = [W.copy()]\n",
    "    T = num_epochs * (Xs.shape[1] // B)\n",
    "    for iter in range(T):  # by pseudocode, T=epochs i guess\n",
    "        W = W - alpha * multinomial_logreg_batch_grad(\n",
    "            Xs, Ys, gamma, W, random.choice(Xs.shape[1], B)\n",
    "        )\n",
    "        if (iter + 1) % monitor_period == 0:\n",
    "            paramsHist.append(W.copy())\n",
    "    return paramsHist\n",
    "\n",
    "\n",
    "# ALGORITHM 2: run stochastic gradient descent with minibatching and sequential sampling order\n",
    "#\n",
    "# Xs              training examples (d * n)\n",
    "# Ys              training labels   (c * n)\n",
    "# gamma           L2 regularization constant\n",
    "# W0              the initial value of the parameters (c * d)\n",
    "# alpha           step size/learning rate\n",
    "# B               minibatch size\n",
    "# num_epochs      number of epochs (passes through the training set) to run\n",
    "# monitor_period  how frequently, in terms of batches (not epochs) to output the parameter vector\n",
    "#\n",
    "# returns         a list of model parameters vectors, one every \"monitor_period\" batches\n",
    "def sgd_minibatch_sequential_scan(\n",
    "    Xs, Ys, gamma, W0, alpha, B, num_epochs, monitor_period\n",
    "):\n",
    "    W = W0\n",
    "    paramsHist = [W.copy()]\n",
    "    n = Xs.shape[1]\n",
    "    for epoch in range(num_epochs):  # by pseudocode, T=epochs i guess\n",
    "        for iter in range(n // B):\n",
    "            W = W - alpha * multinomial_logreg_batch_grad(\n",
    "                Xs, Ys, gamma, W, range(iter * B, (iter + 1) * B)\n",
    "            )\n",
    "            if ((epoch * (n // B)) + iter + 1) % monitor_period == 0:\n",
    "                paramsHist.append(W.copy())\n",
    "    return paramsHist\n",
    "\n",
    "\n",
    "# ALGORITHM 3: run stochastic gradient descent with minibatching and without-replacement sampling\n",
    "#\n",
    "# Xs              training examples (d * n)\n",
    "# Ys              training labels   (c * n)\n",
    "# gamma           L2 regularization constant\n",
    "# W0              the initial value of the parameters (c * d)\n",
    "# alpha           step size/learning rate\n",
    "# B               minibatch size\n",
    "# num_epochs      number of epochs (passes through the training set) to run\n",
    "# monitor_period  how frequently, in terms of batches (not epochs) to output the parameter vector\n",
    "#\n",
    "# returns         a list of model parameters vectors, one every \"monitor_period\" batches\n",
    "def sgd_minibatch_random_reshuffling(\n",
    "    Xs, Ys, gamma, W0, alpha, B, num_epochs, monitor_period\n",
    "):\n",
    "    W = W0\n",
    "    paramsHist = [W.copy()]\n",
    "    n = Xs.shape[1]\n",
    "    shuffledIndices = numpy.arange(n)\n",
    "    for epoch in range(num_epochs):  # by pseudocode, T=epochs i guess\n",
    "        random.shuffle(shuffledIndices)\n",
    "        for iter in range(n // B):\n",
    "            W = W - alpha * multinomial_logreg_batch_grad(\n",
    "                Xs, Ys, gamma, W, shuffledIndices[iter * B : (iter + 1) * B]\n",
    "            )\n",
    "            if ((epoch * (n // B)) + iter + 1) % monitor_period == 0:\n",
    "                paramsHist.append(W.copy())\n",
    "    return paramsHist\n",
    "\n",
    "\n",
    "def timeit(f):\n",
    "    start = time()\n",
    "    val = f()\n",
    "    return val, time() - start\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    (Xs_tr, Ys_tr, Xs_te, Ys_te) = load_MNIST_dataset()\n",
    "\n",
    "    params = 2 * random.rand(Ys_tr.shape[0], Xs_tr.shape[0]) - 1\n",
    "    gd = lambda: gradient_descent(\n",
    "        Xs_tr,\n",
    "        Ys_tr,\n",
    "        0.0001,\n",
    "        2 * random.rand(Ys_tr.shape[0], Xs_tr.shape[0]) - 1,\n",
    "        1.0,\n",
    "        1000,\n",
    "        10,\n",
    "    )\n",
    "    paramHist, secs = timeit(gd)\n",
    "    errors = [multinomial_logreg_error(Xs_tr, Ys_tr, param) for param in paramHist]\n",
    "    for i, j in enumerate(errors):\n",
    "        print(f\"Accuracy Measurement {i+1}: {1-j}\")\n",
    "    print(f\"Gradient Descent (1000 Iters) Took {secs} Seconds\")\n",
    "\n",
    "    # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "    #               Three Algorithms\n",
    "    # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "    batch = 25\n",
    "    alpha = 0.1\n",
    "    monitor_period = 6000/batch\n",
    "    gamma = 0.0001\n",
    "    W0 = lambda: 2 * random.rand(Ys_tr.shape[0], Xs_tr.shape[0]) - 1\n",
    "    # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "    #                   Hyperparams\n",
    "    # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "    # SGD\n",
    "    sgd = lambda: sgd_minibatch(\n",
    "        Xs_tr,\n",
    "        Ys_tr,\n",
    "        gamma=gamma,\n",
    "        W0=W0(),\n",
    "        alpha=alpha,\n",
    "        B=batch,\n",
    "        num_epochs=10,\n",
    "        monitor_period=monitor_period,\n",
    "    )\n",
    "    paramHistSGD, secs = timeit(sgd)\n",
    "    errorsSGD = [\n",
    "        multinomial_logreg_error(Xs_tr, Ys_tr, param) for param in paramHistSGD\n",
    "    ]\n",
    "    for i, j in enumerate(errorsSGD):\n",
    "        print(f\"Accuracy Measurement {i+1}: {1-j}\")\n",
    "    print(f\"SGD Normal B={batch}, A={alpha}, gamma={gamma} Took {secs} Seconds\\n\")\n",
    "\n",
    "    # SGD Sequential\n",
    "    params = 2 * random.rand(Ys_tr.shape[0], Xs_tr.shape[0]) - 1\n",
    "    sgds = lambda: sgd_minibatch_sequential_scan(\n",
    "        Xs_tr,\n",
    "        Ys_tr,\n",
    "        gamma=gamma,\n",
    "        W0=W0(),\n",
    "        alpha=alpha,\n",
    "        B=batch,\n",
    "        num_epochs=10,\n",
    "        monitor_period=monitor_period,\n",
    "    )\n",
    "    paramHistSGDS, secs = timeit(sgds)\n",
    "    errorsSGDS = [\n",
    "        multinomial_logreg_error(Xs_tr, Ys_tr, param) for param in paramHistSGDS\n",
    "    ]\n",
    "\n",
    "    for i, j in enumerate(errorsSGDS):\n",
    "        print(f\"Accuracy Measurement {i+1}: {1-j}\")\n",
    "    print(f\"SGD Sequential B={batch}, A={alpha}, gamma={gamma} Took {secs} Seconds\\n\")\n",
    "\n",
    "    # SGD Random Shuffling\n",
    "    params = 2 * random.rand(Ys_tr.shape[0], Xs_tr.shape[0]) - 1\n",
    "    sgdrs = lambda: sgd_minibatch_random_reshuffling(\n",
    "        Xs_tr,\n",
    "        Ys_tr,\n",
    "        gamma=gamma,\n",
    "        W0=W0(),\n",
    "        alpha=alpha,\n",
    "        B=batch,\n",
    "        num_epochs=10,\n",
    "        monitor_period=monitor_period,\n",
    "    )\n",
    "    paramHistSGDRS, secs = timeit(sgdrs)\n",
    "    errorsSGDRS = [\n",
    "        multinomial_logreg_error(Xs_tr, Ys_tr, param) for param in paramHistSGDRS\n",
    "    ]\n",
    "    for i, j in enumerate(errorsSGDRS):\n",
    "        print(f\"Accuracy Measurement {i+1}: {1-j}\")\n",
    "    print(\n",
    "        f\"SGD Random Shuffling B={batch}, A={alpha}, gamma={gamma} Took {secs} Seconds\\n\"\n",
    "    )\n",
    "    # GRAPHING\n",
    "    # gradient descent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9141333333333334\n",
      "SGD Sequential B=60, A=0.3, gamma=0.0001 Took 3.916424036026001 Seconds\n",
      "\n"
     ]
    }
   ],
   "source": [
    "batch = 60\n",
    "alpha=0.3\n",
    "monitor_period = 6000/batch\n",
    "gamma = 0.0001\n",
    "W0 = lambda: 2 * random.rand(Ys_tr.shape[0], Xs_tr.shape[0]) - 1\n",
    "# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "#                   Hyperparams\n",
    "# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "# SGD\n",
    "sgd = lambda: sgd_minibatch(\n",
    "    Xs_tr,\n",
    "    Ys_tr,\n",
    "    gamma=gamma,\n",
    "    W0=W0(),\n",
    "    alpha=alpha,\n",
    "    B=batch,\n",
    "    num_epochs=5,\n",
    "    monitor_period=monitor_period,\n",
    ")\n",
    "paramHistSGD, secs = timeit(sgd)\n",
    "print(1-multinomial_logreg_error(Xs_tr, Ys_tr, paramHistSGD[-1]))\n",
    "print(f\"SGD Sequential B={batch}, A={alpha}, gamma={gamma} Took {secs} Seconds\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "iters = [( i + 1) for i in range(5)]\n",
    "trErrorSGD = [multinomial_logreg_error(Xs_tr, Ys_tr, weight) for weight in paramHistSGD[1::10]]\n",
    "teErrorSGD = [multinomial_logreg_error(Xs_te, Ys_te, weight) for weight in paramHistSGD[1::10]]\n",
    "\n",
    "trErrorSGDS = [multinomial_logreg_error(Xs_tr, Ys_tr, weight) for weight in paramHistSGDS[1::10]]\n",
    "teErrorSGDS = [multinomial_logreg_error(Xs_te, Ys_te, weight) for weight in paramHistSGDS[1::10]]\n",
    "\n",
    "trErrorSGDRS = [multinomial_logreg_error(Xs_tr, Ys_tr, weight) for weight in paramHistSGDRS[1::10]]\n",
    "teErrorSGDRS = [multinomial_logreg_error(Xs_te, Ys_te, weight) for weight in paramHistSGDRS[1::10]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, axs = pyplot.subplots(2,1)\n",
    "axs[1].plot(iters, teErrorSGD)\n",
    "axs[1].set_title(\"Testing Error\")\n",
    "axs[0].plot(iters, trErrorSGD, \"tab:green\")\n",
    "axs[0].set_title(\"Training Error\")\n",
    "# axs[1, 0].plot(iters, trLossGD)\n",
    "# axs[1, 0].set_title(\"Training Loss\")\n",
    "# axs[0, 0].plot(iters, trErrorGD, \"tab:orange\")\n",
    "# axs[0, 0].set_title(\"Training Error\")\n",
    "# axs[1, 1].plot(iters, teLossGD, \"tab:green\")\n",
    "# axs[1, 1].set_title(\"Test Loss\")\n",
    "# axs[0, 1].plot(iters, teErrorGD, \"tab:red\")\n",
    "# axs[0, 1].set_title(\"Test Error\")\n",
    "\n",
    "for ax in axs:\n",
    "    ax.set(xlabel=\"Epoch\", ylabel=\"Error\")\n",
    "# for ax in axs[1]:\n",
    "#     ax.set(xlabel=\"Iteration\", ylabel=\"Loss\")\n",
    "fig.suptitle('Error/Loss for Stochastic Gradient Descent Normal B = 60, alpha = 0.3, 5 Epochs') \n",
    "# Hide x labels and tick labels for top plots and y ticks for right plots.\n",
    "for ax in axs.flat:\n",
    "    ax.label_outer()\n",
    "pyplot.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
