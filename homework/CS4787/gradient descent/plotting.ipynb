{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Measurement 1: 0.10743333333333338\n",
      "Accuracy Measurement 2: 0.42513333333333336\n",
      "Accuracy Measurement 3: 0.5903666666666667\n",
      "Accuracy Measurement 4: 0.6691833333333334\n",
      "Accuracy Measurement 5: 0.7132333333333334\n",
      "Accuracy Measurement 6: 0.7450333333333333\n",
      "Accuracy Measurement 7: 0.7667666666666667\n",
      "Accuracy Measurement 8: 0.7835333333333333\n",
      "Accuracy Measurement 9: 0.79565\n",
      "Accuracy Measurement 10: 0.8053333333333333\n",
      "Accuracy Measurement 11: 0.8135833333333333\n",
      "Accuracy Measurement 12: 0.8206833333333333\n",
      "Accuracy Measurement 13: 0.8282166666666667\n",
      "Accuracy Measurement 14: 0.8317666666666667\n",
      "Accuracy Measurement 15: 0.8360833333333333\n",
      "Accuracy Measurement 16: 0.8411666666666666\n",
      "Accuracy Measurement 17: 0.8448333333333333\n",
      "Accuracy Measurement 18: 0.84815\n",
      "Accuracy Measurement 19: 0.85035\n",
      "Accuracy Measurement 20: 0.85275\n",
      "Accuracy Measurement 21: 0.8542\n",
      "Accuracy Measurement 22: 0.8561666666666666\n",
      "Accuracy Measurement 23: 0.8580666666666666\n",
      "Accuracy Measurement 24: 0.8609333333333333\n",
      "Accuracy Measurement 25: 0.8604166666666667\n",
      "Accuracy Measurement 26: 0.8632666666666666\n",
      "Accuracy Measurement 27: 0.8641833333333333\n",
      "Accuracy Measurement 28: 0.866\n",
      "Accuracy Measurement 29: 0.8674333333333333\n",
      "Accuracy Measurement 30: 0.8691166666666666\n",
      "Accuracy Measurement 31: 0.8703833333333333\n",
      "Accuracy Measurement 32: 0.8723\n",
      "Accuracy Measurement 33: 0.8730666666666667\n",
      "Accuracy Measurement 34: 0.8745333333333334\n",
      "Accuracy Measurement 35: 0.8749\n",
      "Accuracy Measurement 36: 0.8757\n",
      "Accuracy Measurement 37: 0.8771\n",
      "Accuracy Measurement 38: 0.8771\n",
      "Accuracy Measurement 39: 0.8777166666666667\n",
      "Accuracy Measurement 40: 0.8792333333333333\n",
      "Accuracy Measurement 41: 0.8804666666666666\n",
      "Accuracy Measurement 42: 0.8812333333333333\n",
      "Accuracy Measurement 43: 0.8818\n",
      "Accuracy Measurement 44: 0.8833\n",
      "Accuracy Measurement 45: 0.8835666666666666\n",
      "Accuracy Measurement 46: 0.8845666666666666\n",
      "Accuracy Measurement 47: 0.8844666666666666\n",
      "Accuracy Measurement 48: 0.8855833333333333\n",
      "Accuracy Measurement 49: 0.8856666666666667\n",
      "Accuracy Measurement 50: 0.886\n",
      "Accuracy Measurement 51: 0.8874166666666666\n",
      "Accuracy Measurement 52: 0.8874\n",
      "Accuracy Measurement 53: 0.88845\n",
      "Accuracy Measurement 54: 0.8887666666666667\n",
      "Accuracy Measurement 55: 0.88885\n",
      "Accuracy Measurement 56: 0.8892833333333333\n",
      "Accuracy Measurement 57: 0.8905166666666666\n",
      "Accuracy Measurement 58: 0.8908166666666667\n",
      "Accuracy Measurement 59: 0.89095\n",
      "Accuracy Measurement 60: 0.8916166666666666\n",
      "Accuracy Measurement 61: 0.8923833333333333\n",
      "Accuracy Measurement 62: 0.8924833333333333\n",
      "Accuracy Measurement 63: 0.89275\n",
      "Accuracy Measurement 64: 0.8933166666666666\n",
      "Accuracy Measurement 65: 0.8932833333333333\n",
      "Accuracy Measurement 66: 0.89365\n",
      "Accuracy Measurement 67: 0.8939333333333334\n",
      "Accuracy Measurement 68: 0.89435\n",
      "Accuracy Measurement 69: 0.8945333333333333\n",
      "Accuracy Measurement 70: 0.8951\n",
      "Accuracy Measurement 71: 0.8946833333333334\n",
      "Accuracy Measurement 72: 0.8964666666666666\n",
      "Accuracy Measurement 73: 0.8967833333333334\n",
      "Accuracy Measurement 74: 0.8961833333333333\n",
      "Accuracy Measurement 75: 0.89745\n",
      "Accuracy Measurement 76: 0.8969333333333334\n",
      "Accuracy Measurement 77: 0.8968\n",
      "Accuracy Measurement 78: 0.8974166666666666\n",
      "Accuracy Measurement 79: 0.8979166666666667\n",
      "Accuracy Measurement 80: 0.8989166666666667\n",
      "Accuracy Measurement 81: 0.89785\n",
      "Accuracy Measurement 82: 0.8979333333333334\n",
      "Accuracy Measurement 83: 0.8974333333333333\n",
      "Accuracy Measurement 84: 0.8988333333333334\n",
      "Accuracy Measurement 85: 0.89855\n",
      "Accuracy Measurement 86: 0.8988833333333334\n",
      "Accuracy Measurement 87: 0.8992833333333333\n",
      "Accuracy Measurement 88: 0.8999333333333334\n",
      "Accuracy Measurement 89: 0.8998\n",
      "Accuracy Measurement 90: 0.9005333333333333\n",
      "Accuracy Measurement 91: 0.9001666666666667\n",
      "Accuracy Measurement 92: 0.9011666666666667\n",
      "Accuracy Measurement 93: 0.9010666666666667\n",
      "Accuracy Measurement 94: 0.9010333333333334\n",
      "Accuracy Measurement 95: 0.9018666666666667\n",
      "Accuracy Measurement 96: 0.90205\n",
      "Accuracy Measurement 97: 0.9020166666666667\n",
      "Accuracy Measurement 98: 0.9014833333333333\n",
      "Accuracy Measurement 99: 0.9029833333333334\n",
      "Accuracy Measurement 100: 0.9024333333333333\n",
      "Accuracy Measurement 101: 0.9027166666666666\n",
      "SGD Normal B=1, A=0.001, gamma=0.0001 Took 27.719687938690186 Seconds\n",
      "\n",
      "Accuracy Measurement 1: 0.05668333333333331\n",
      "Accuracy Measurement 2: 0.4454\n",
      "Accuracy Measurement 3: 0.60325\n",
      "Accuracy Measurement 4: 0.6808833333333333\n",
      "Accuracy Measurement 5: 0.7257166666666667\n",
      "Accuracy Measurement 6: 0.75155\n",
      "Accuracy Measurement 7: 0.7715833333333333\n",
      "Accuracy Measurement 8: 0.7858333333333334\n",
      "Accuracy Measurement 9: 0.7948833333333334\n",
      "Accuracy Measurement 10: 0.8056333333333333\n",
      "Accuracy Measurement 11: 0.81305\n",
      "Accuracy Measurement 12: 0.81915\n",
      "Accuracy Measurement 13: 0.8235833333333333\n",
      "Accuracy Measurement 14: 0.82905\n",
      "Accuracy Measurement 15: 0.834\n",
      "Accuracy Measurement 16: 0.8382666666666667\n",
      "Accuracy Measurement 17: 0.8404666666666667\n",
      "Accuracy Measurement 18: 0.8428\n",
      "Accuracy Measurement 19: 0.8464\n",
      "Accuracy Measurement 20: 0.8482666666666666\n",
      "Accuracy Measurement 21: 0.85185\n",
      "Accuracy Measurement 22: 0.8537166666666667\n",
      "Accuracy Measurement 23: 0.8555166666666667\n",
      "Accuracy Measurement 24: 0.85725\n",
      "Accuracy Measurement 25: 0.85975\n",
      "Accuracy Measurement 26: 0.8621666666666666\n",
      "Accuracy Measurement 27: 0.8631833333333333\n",
      "Accuracy Measurement 28: 0.8635833333333334\n",
      "Accuracy Measurement 29: 0.8657166666666667\n",
      "Accuracy Measurement 30: 0.86695\n",
      "Accuracy Measurement 31: 0.869\n",
      "Accuracy Measurement 32: 0.8705333333333334\n",
      "Accuracy Measurement 33: 0.8715666666666667\n",
      "Accuracy Measurement 34: 0.8723833333333333\n",
      "Accuracy Measurement 35: 0.87335\n",
      "Accuracy Measurement 36: 0.8747833333333334\n",
      "Accuracy Measurement 37: 0.8754666666666666\n",
      "Accuracy Measurement 38: 0.8759166666666667\n",
      "Accuracy Measurement 39: 0.87745\n",
      "Accuracy Measurement 40: 0.8772166666666666\n",
      "Accuracy Measurement 41: 0.8792166666666666\n",
      "Accuracy Measurement 42: 0.8804166666666666\n",
      "Accuracy Measurement 43: 0.8807833333333334\n",
      "Accuracy Measurement 44: 0.88165\n",
      "Accuracy Measurement 45: 0.88165\n",
      "Accuracy Measurement 46: 0.88315\n",
      "Accuracy Measurement 47: 0.8837666666666667\n",
      "Accuracy Measurement 48: 0.88345\n",
      "Accuracy Measurement 49: 0.8849\n",
      "Accuracy Measurement 50: 0.8846666666666667\n",
      "Accuracy Measurement 51: 0.8859833333333333\n",
      "Accuracy Measurement 52: 0.8867833333333334\n",
      "Accuracy Measurement 53: 0.8875333333333333\n",
      "Accuracy Measurement 54: 0.8879833333333333\n",
      "Accuracy Measurement 55: 0.8879\n",
      "Accuracy Measurement 56: 0.8895\n",
      "Accuracy Measurement 57: 0.889\n",
      "Accuracy Measurement 58: 0.8889\n",
      "Accuracy Measurement 59: 0.8899333333333334\n",
      "Accuracy Measurement 60: 0.89105\n",
      "Accuracy Measurement 61: 0.89125\n",
      "Accuracy Measurement 62: 0.89135\n",
      "Accuracy Measurement 63: 0.8915333333333333\n",
      "Accuracy Measurement 64: 0.893\n",
      "Accuracy Measurement 65: 0.8923\n",
      "Accuracy Measurement 66: 0.89415\n",
      "Accuracy Measurement 67: 0.8934666666666666\n",
      "Accuracy Measurement 68: 0.8929166666666667\n",
      "Accuracy Measurement 69: 0.894\n",
      "Accuracy Measurement 70: 0.8949833333333334\n",
      "Accuracy Measurement 71: 0.8950333333333333\n",
      "Accuracy Measurement 72: 0.89525\n",
      "Accuracy Measurement 73: 0.8955166666666666\n",
      "Accuracy Measurement 74: 0.89655\n",
      "Accuracy Measurement 75: 0.8955333333333333\n",
      "Accuracy Measurement 76: 0.8974166666666666\n",
      "Accuracy Measurement 77: 0.89675\n",
      "Accuracy Measurement 78: 0.8961666666666667\n",
      "Accuracy Measurement 79: 0.8977166666666667\n",
      "Accuracy Measurement 80: 0.8980166666666667\n",
      "Accuracy Measurement 81: 0.898\n",
      "Accuracy Measurement 82: 0.89845\n",
      "Accuracy Measurement 83: 0.89835\n",
      "Accuracy Measurement 84: 0.8992666666666667\n",
      "Accuracy Measurement 85: 0.8984833333333333\n",
      "Accuracy Measurement 86: 0.9001166666666667\n",
      "Accuracy Measurement 87: 0.89965\n",
      "Accuracy Measurement 88: 0.8988666666666667\n",
      "Accuracy Measurement 89: 0.9002\n",
      "Accuracy Measurement 90: 0.90085\n",
      "Accuracy Measurement 91: 0.9001166666666667\n",
      "Accuracy Measurement 92: 0.9012\n",
      "Accuracy Measurement 93: 0.9007833333333334\n",
      "Accuracy Measurement 94: 0.9017\n",
      "Accuracy Measurement 95: 0.9010166666666667\n",
      "Accuracy Measurement 96: 0.90245\n",
      "Accuracy Measurement 97: 0.9020833333333333\n",
      "Accuracy Measurement 98: 0.9011833333333333\n",
      "Accuracy Measurement 99: 0.9025333333333333\n",
      "Accuracy Measurement 100: 0.9029666666666667\n",
      "Accuracy Measurement 101: 0.9019666666666667\n",
      "SGD Sequential B=1, A=0.001, gamma=0.0001 Took 24.439671993255615 Seconds\n",
      "\n",
      "Accuracy Measurement 1: 0.1372833333333333\n",
      "Accuracy Measurement 2: 0.47035000000000005\n",
      "Accuracy Measurement 3: 0.6238166666666667\n",
      "Accuracy Measurement 4: 0.6875166666666667\n",
      "Accuracy Measurement 5: 0.7255333333333334\n",
      "Accuracy Measurement 6: 0.75025\n",
      "Accuracy Measurement 7: 0.77035\n",
      "Accuracy Measurement 8: 0.785\n",
      "Accuracy Measurement 9: 0.7959166666666667\n",
      "Accuracy Measurement 10: 0.8029\n",
      "Accuracy Measurement 11: 0.8136833333333333\n",
      "Accuracy Measurement 12: 0.81905\n",
      "Accuracy Measurement 13: 0.8239\n",
      "Accuracy Measurement 14: 0.8285666666666667\n",
      "Accuracy Measurement 15: 0.8324666666666667\n",
      "Accuracy Measurement 16: 0.8378833333333333\n",
      "Accuracy Measurement 17: 0.8403833333333334\n",
      "Accuracy Measurement 18: 0.8434166666666667\n",
      "Accuracy Measurement 19: 0.84715\n",
      "Accuracy Measurement 20: 0.8501333333333333\n",
      "Accuracy Measurement 21: 0.8524333333333334\n",
      "Accuracy Measurement 22: 0.8547166666666667\n",
      "Accuracy Measurement 23: 0.8553666666666667\n",
      "Accuracy Measurement 24: 0.8593\n",
      "Accuracy Measurement 25: 0.8602833333333333\n",
      "Accuracy Measurement 26: 0.8616833333333334\n",
      "Accuracy Measurement 27: 0.86345\n",
      "Accuracy Measurement 28: 0.8641333333333333\n",
      "Accuracy Measurement 29: 0.86585\n",
      "Accuracy Measurement 30: 0.8677333333333334\n",
      "Accuracy Measurement 31: 0.8686166666666667\n",
      "Accuracy Measurement 32: 0.8699666666666667\n",
      "Accuracy Measurement 33: 0.8705\n",
      "Accuracy Measurement 34: 0.8719666666666667\n",
      "Accuracy Measurement 35: 0.8728666666666667\n",
      "Accuracy Measurement 36: 0.8742666666666666\n",
      "Accuracy Measurement 37: 0.8754666666666666\n",
      "Accuracy Measurement 38: 0.8759\n",
      "Accuracy Measurement 39: 0.8766\n",
      "Accuracy Measurement 40: 0.8773\n",
      "Accuracy Measurement 41: 0.8773666666666666\n",
      "Accuracy Measurement 42: 0.8786333333333334\n",
      "Accuracy Measurement 43: 0.8799333333333333\n",
      "Accuracy Measurement 44: 0.8801833333333333\n",
      "Accuracy Measurement 45: 0.8808\n",
      "Accuracy Measurement 46: 0.8816\n",
      "Accuracy Measurement 47: 0.8809833333333333\n",
      "Accuracy Measurement 48: 0.8827333333333334\n",
      "Accuracy Measurement 49: 0.88335\n",
      "Accuracy Measurement 50: 0.88285\n",
      "Accuracy Measurement 51: 0.88415\n",
      "Accuracy Measurement 52: 0.88495\n",
      "Accuracy Measurement 53: 0.8856666666666667\n",
      "Accuracy Measurement 54: 0.8852\n",
      "Accuracy Measurement 55: 0.88585\n",
      "Accuracy Measurement 56: 0.8868833333333334\n",
      "Accuracy Measurement 57: 0.8868833333333334\n",
      "Accuracy Measurement 58: 0.8883166666666666\n",
      "Accuracy Measurement 59: 0.88725\n",
      "Accuracy Measurement 60: 0.8887166666666667\n",
      "Accuracy Measurement 61: 0.8891833333333333\n",
      "Accuracy Measurement 62: 0.8903666666666666\n",
      "Accuracy Measurement 63: 0.8906\n",
      "Accuracy Measurement 64: 0.8899\n",
      "Accuracy Measurement 65: 0.8890833333333333\n",
      "Accuracy Measurement 66: 0.89095\n",
      "Accuracy Measurement 67: 0.89115\n",
      "Accuracy Measurement 68: 0.8922333333333333\n",
      "Accuracy Measurement 69: 0.8923833333333333\n",
      "Accuracy Measurement 70: 0.8923833333333333\n",
      "Accuracy Measurement 71: 0.8930833333333333\n",
      "Accuracy Measurement 72: 0.89285\n",
      "Accuracy Measurement 73: 0.8933\n",
      "Accuracy Measurement 74: 0.8941\n",
      "Accuracy Measurement 75: 0.8942666666666667\n",
      "Accuracy Measurement 76: 0.8945833333333333\n",
      "Accuracy Measurement 77: 0.8956166666666666\n",
      "Accuracy Measurement 78: 0.8954666666666666\n",
      "Accuracy Measurement 79: 0.8953666666666666\n",
      "Accuracy Measurement 80: 0.8957\n",
      "Accuracy Measurement 81: 0.8961666666666667\n",
      "Accuracy Measurement 82: 0.8965833333333333\n",
      "Accuracy Measurement 83: 0.8969\n",
      "Accuracy Measurement 84: 0.8964333333333333\n",
      "Accuracy Measurement 85: 0.8972666666666667\n",
      "Accuracy Measurement 86: 0.89745\n",
      "Accuracy Measurement 87: 0.89715\n",
      "Accuracy Measurement 88: 0.898\n",
      "Accuracy Measurement 89: 0.8978666666666667\n",
      "Accuracy Measurement 90: 0.8984833333333333\n",
      "Accuracy Measurement 91: 0.8985166666666666\n",
      "Accuracy Measurement 92: 0.8986833333333333\n",
      "Accuracy Measurement 93: 0.89895\n",
      "Accuracy Measurement 94: 0.8995833333333333\n",
      "Accuracy Measurement 95: 0.9001333333333333\n",
      "Accuracy Measurement 96: 0.9001833333333333\n",
      "Accuracy Measurement 97: 0.8998166666666667\n",
      "Accuracy Measurement 98: 0.9003333333333333\n",
      "Accuracy Measurement 99: 0.9012\n",
      "Accuracy Measurement 100: 0.9010666666666667\n",
      "Accuracy Measurement 101: 0.9008166666666667\n",
      "SGD Random Shuffling B=1, A=0.001, gamma=0.0001 Took 25.456284284591675 Seconds\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy\n",
    "from numpy import random\n",
    "import scipy\n",
    "from scipy.special import softmax\n",
    "import mnist\n",
    "import pickle\n",
    "\n",
    "# you can use matplotlib for plotting\n",
    "from time import time\n",
    "import matplotlib\n",
    "from matplotlib import pyplot\n",
    "\n",
    "mnist_data_directory = os.path.join(os.path.abspath(''), \"data\")\n",
    "\n",
    "# TODO add any additional imports and global variables\n",
    "\n",
    "\n",
    "def load_MNIST_dataset():\n",
    "    PICKLE_FILE = os.path.join(mnist_data_directory, \"MNIST.pickle\")\n",
    "    try:\n",
    "        dataset = pickle.load(open(PICKLE_FILE, \"rb\"))\n",
    "    except:\n",
    "        # load the MNIST dataset\n",
    "        mnist_data = mnist.MNIST(mnist_data_directory, return_type=\"numpy\", gz=True)\n",
    "        Xs_tr, Lbls_tr = mnist_data.load_training()\n",
    "        Xs_tr = Xs_tr.transpose() / 255.0\n",
    "        Ys_tr = numpy.zeros((10, 60000))\n",
    "        for i in range(60000):\n",
    "            Ys_tr[Lbls_tr[i], i] = 1.0  # one-hot encode each label\n",
    "        Xs_tr = numpy.ascontiguousarray(Xs_tr)\n",
    "        Ys_tr = numpy.ascontiguousarray(Ys_tr)\n",
    "        Xs_te, Lbls_te = mnist_data.load_testing()\n",
    "        Xs_te = Xs_te.transpose() / 255.0\n",
    "        Ys_te = numpy.zeros((10, 10000))\n",
    "        for i in range(10000):\n",
    "            Ys_te[Lbls_te[i], i] = 1.0  # one-hot encode each label\n",
    "        Xs_te = numpy.ascontiguousarray(Xs_te)\n",
    "        Ys_te = numpy.ascontiguousarray(Ys_te)\n",
    "        dataset = (Xs_tr, Ys_tr, Xs_te, Ys_te)\n",
    "        pickle.dump(dataset, open(PICKLE_FILE, \"wb\"))\n",
    "    return dataset\n",
    "\n",
    "\n",
    "# compute the cross-entropy loss of the classifier\n",
    "#\n",
    "# x         examples          (d)\n",
    "# y         labels            (c)\n",
    "# gamma     L2 regularization constant\n",
    "# W         parameters        (c * d)\n",
    "#\n",
    "# returns   the model cross-entropy loss\n",
    "def multinomial_logreg_loss_i(x, y, gamma, W):\n",
    "    return numpy.dot(y, -numpy.log(softmax(W @ x)))\n",
    "\n",
    "\n",
    "# compute the gradient of a single example of the multinomial logistic regression objective, with regularization\n",
    "#\n",
    "# x         training example   (d)\n",
    "# y         training label     (c)\n",
    "# gamma     L2 regularization constant\n",
    "# W         parameters        (c * d)\n",
    "#\n",
    "# returns   the gradient of the loss with respect to the model parameters W\n",
    "def multinomial_logreg_grad_i(x, y, gamma, W):\n",
    "    return numpy.outer(softmax(W @ x) - y, x) + gamma * W\n",
    "\n",
    "\n",
    "# compute the error of the classifier\n",
    "#\n",
    "# Xs        examples          (d * n)\n",
    "# Ys        labels            (c * n)\n",
    "# W         parameters        (c * d)\n",
    "#\n",
    "# returns   the model error as a percentage of incorrect labels\n",
    "def multinomial_logreg_error(Xs, Ys, W):\n",
    "    val = softmax(W @ Xs)\n",
    "    preds = numpy.zeros_like(val)\n",
    "    preds[numpy.argmax(val, axis=0), range(val.shape[1])] = 1\n",
    "    return 1 - numpy.sum(preds * Ys) / Ys.shape[1]\n",
    "\n",
    "\n",
    "# compute the gradient of the multinomial logistic regression objective on a batch, with regularization\n",
    "#\n",
    "# Xs        training examples (d * n)\n",
    "# Ys        training labels   (c * n)\n",
    "# gamma     L2 regularization constant\n",
    "# W         parameters        (c * d)\n",
    "# ii        indices of the batch (an iterable or range)\n",
    "#\n",
    "# returns   the gradient of the model parameters\n",
    "def multinomial_logreg_batch_grad(Xs, Ys, gamma, W, ii=None):\n",
    "    if ii is None:\n",
    "        ii = range(Xs.shape[1])\n",
    "        X = Xs\n",
    "        Y = Ys\n",
    "    else:\n",
    "        X = Xs[:, ii]\n",
    "        Y = Ys[:, ii]\n",
    "    # a starter solution using an average of the example gradients\n",
    "    (d, n) = Xs.shape\n",
    "    return ((softmax(W @ X, axis=0) - Y) @ X.T) / len(ii) + gamma * W\n",
    "\n",
    "\n",
    "# compute the cross-entropy loss of the classifier on a batch, with regularization\n",
    "#\n",
    "# Xs        examples          (d * n)\n",
    "# Ys        labels            (c * n)\n",
    "# gamma     L2 regularization constant\n",
    "# W         parameters        (c * d)\n",
    "# ii        indices of the batch (an iterable or range)\n",
    "#\n",
    "# returns   the model cross-entropy loss\n",
    "def multinomial_logreg_batch_loss(Xs, Ys, gamma, W, ii=None):\n",
    "    if ii is None:\n",
    "        ii = range(Xs.shape[1])\n",
    "        X = Xs\n",
    "        Y = Ys\n",
    "    else:\n",
    "        X = Xs[:, ii]\n",
    "        Y = Ys[:, ii]\n",
    "\n",
    "    (d, n) = Xs.shape\n",
    "    frobNorm = numpy.linalg.norm(W) ** 2\n",
    "    return (\n",
    "        numpy.sum(Y * (-numpy.log(softmax(W @ X, axis=0)))) / len(ii)\n",
    "        + gamma * frobNorm / 2\n",
    "    )\n",
    "\n",
    "    # acc = 0.0\n",
    "    # for i in ii:\n",
    "    #     acc += multinomial_logreg_loss_i(Xs[:, i], Ys[:, i], gamma, W)\n",
    "    # return acc / len(ii)\n",
    "\n",
    "\n",
    "# run gradient descent on a multinomial logistic regression objective, with regularization\n",
    "#\n",
    "# Xs            training examples (d * n)\n",
    "# Ys            training labels   (d * c)\n",
    "# gamma         L2 regularization constant\n",
    "# W0            the initial value of the parameters (c * d)\n",
    "# alpha         step size/learning rate\n",
    "# num_iters     number of iterations to run\n",
    "# monitor_freq  how frequently to output the parameter vector\n",
    "#\n",
    "# returns       a list of models parameters, one every \"monitor_freq\" iterations\n",
    "#               should return model parameters before iteration 0, iteration monitor_freq, iteration 2*monitor_freq, and again at the end\n",
    "#               for a total of (num_iters/monitor_freq)+1 models, if num_iters is divisible by monitor_freq.\n",
    "def gradient_descent(Xs, Ys, gamma, W0, alpha, num_iters, monitor_freq):\n",
    "    W = W0\n",
    "    paramsHist = [W.copy()]\n",
    "    for iter in range(num_iters):\n",
    "        W = W - alpha * multinomial_logreg_batch_grad(Xs, Ys, gamma, W, None)\n",
    "        if (iter + 1) % monitor_freq == 0:\n",
    "            paramsHist.append(W.copy())\n",
    "    return paramsHist\n",
    "\n",
    "\n",
    "# ALGORITHM 1: run stochastic gradient descent on a multinomial logistic regression objective, with regularization\n",
    "#\n",
    "# Xs              training examples (d * n)\n",
    "# Ys              training labels   (c * n)\n",
    "# gamma           L2 regularization constant\n",
    "# W0              the initial value of the parameters (c * d)\n",
    "# alpha           step size/learning rate\n",
    "# B               minibatch size\n",
    "# num_epochs      number of epochs (passes through the training set) to run\n",
    "# monitor_period  how frequently, in terms of batches (not epochs) to output the parameter vector\n",
    "#\n",
    "# returns         a list of model parameters vectors, one every \"monitor_period\" batches\n",
    "#                   to do this, you'll want code like the following:\n",
    "#                     models = []\n",
    "#                     models.append(W0.copy())   # (you may not need the copy if you don't mutate W0)\n",
    "#                     ...\n",
    "#                     for sgd_iteration in ... :\n",
    "#                       ...\n",
    "#                       # code to compute a single SGD update step here\n",
    "#                       ...\n",
    "#                       if (it % monitor_period == 0):\n",
    "#                         models.append(W)\n",
    "def sgd_minibatch(Xs, Ys, gamma, W0, alpha, B, num_epochs, monitor_period):\n",
    "    W = W0\n",
    "    paramsHist = [W.copy()]\n",
    "    T = num_epochs * (Xs.shape[1] // B)\n",
    "    for iter in range(T):  # by pseudocode, T=epochs i guess\n",
    "        W = W - alpha * multinomial_logreg_batch_grad(\n",
    "            Xs, Ys, gamma, W, random.choice(Xs.shape[1], B)\n",
    "        )\n",
    "        if (iter + 1) % monitor_period == 0:\n",
    "            paramsHist.append(W.copy())\n",
    "    return paramsHist\n",
    "\n",
    "\n",
    "# ALGORITHM 2: run stochastic gradient descent with minibatching and sequential sampling order\n",
    "#\n",
    "# Xs              training examples (d * n)\n",
    "# Ys              training labels   (c * n)\n",
    "# gamma           L2 regularization constant\n",
    "# W0              the initial value of the parameters (c * d)\n",
    "# alpha           step size/learning rate\n",
    "# B               minibatch size\n",
    "# num_epochs      number of epochs (passes through the training set) to run\n",
    "# monitor_period  how frequently, in terms of batches (not epochs) to output the parameter vector\n",
    "#\n",
    "# returns         a list of model parameters vectors, one every \"monitor_period\" batches\n",
    "def sgd_minibatch_sequential_scan(\n",
    "    Xs, Ys, gamma, W0, alpha, B, num_epochs, monitor_period\n",
    "):\n",
    "    W = W0\n",
    "    paramsHist = [W.copy()]\n",
    "    n = Xs.shape[1]\n",
    "    for epoch in range(num_epochs):  # by pseudocode, T=epochs i guess\n",
    "        for iter in range(n // B):\n",
    "            W = W - alpha * multinomial_logreg_batch_grad(\n",
    "                Xs, Ys, gamma, W, range(iter * B, (iter + 1) * B)\n",
    "            )\n",
    "            if ((epoch * (n // B)) + iter + 1) % monitor_period == 0:\n",
    "                paramsHist.append(W.copy())\n",
    "    return paramsHist\n",
    "\n",
    "\n",
    "# ALGORITHM 3: run stochastic gradient descent with minibatching and without-replacement sampling\n",
    "#\n",
    "# Xs              training examples (d * n)\n",
    "# Ys              training labels   (c * n)\n",
    "# gamma           L2 regularization constant\n",
    "# W0              the initial value of the parameters (c * d)\n",
    "# alpha           step size/learning rate\n",
    "# B               minibatch size\n",
    "# num_epochs      number of epochs (passes through the training set) to run\n",
    "# monitor_period  how frequently, in terms of batches (not epochs) to output the parameter vector\n",
    "#\n",
    "# returns         a list of model parameters vectors, one every \"monitor_period\" batches\n",
    "def sgd_minibatch_random_reshuffling(\n",
    "    Xs, Ys, gamma, W0, alpha, B, num_epochs, monitor_period\n",
    "):\n",
    "    W = W0\n",
    "    paramsHist = [W.copy()]\n",
    "    n = Xs.shape[1]\n",
    "    shuffledIndices = numpy.arange(n)\n",
    "    for epoch in range(num_epochs):  # by pseudocode, T=epochs i guess\n",
    "        random.shuffle(shuffledIndices)\n",
    "        for iter in range(n // B):\n",
    "            W = W - alpha * multinomial_logreg_batch_grad(\n",
    "                Xs, Ys, gamma, W, shuffledIndices[iter * B : (iter + 1) * B]\n",
    "            )\n",
    "            if ((epoch * (n // B)) + iter + 1) % monitor_period == 0:\n",
    "                paramsHist.append(W.copy())\n",
    "    return paramsHist\n",
    "\n",
    "\n",
    "def timeit(f):\n",
    "    start = time()\n",
    "    val = f()\n",
    "    return val, time() - start\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    (Xs_tr, Ys_tr, Xs_te, Ys_te) = load_MNIST_dataset()\n",
    "\n",
    "    # params = 2 * random.rand(Ys_tr.shape[0], Xs_tr.shape[0]) - 1\n",
    "    # gd = lambda: gradient_descent(\n",
    "    #     Xs_tr,\n",
    "    #     Ys_tr,\n",
    "    #     0.0001,\n",
    "    #     2 * random.rand(Ys_tr.shape[0], Xs_tr.shape[0]) - 1,\n",
    "    #     1.0,\n",
    "    #     1000,\n",
    "    #     10,\n",
    "    # )\n",
    "    # paramHist, secs = timeit(gd)\n",
    "    # errors = [multinomial_logreg_error(Xs_tr, Ys_tr, param) for param in paramHist]\n",
    "    # for i, j in enumerate(errors):\n",
    "    #     print(f\"Accuracy Measurement {i+1}: {1-j}\")\n",
    "    # print(f\"Gradient Descent (1000 Iters) Took {secs} Seconds\")\n",
    "\n",
    "    # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "    #               Three Algorithms\n",
    "    # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "    batch = 1\n",
    "    alpha = 0.001\n",
    "    monitor_period = 6000\n",
    "    gamma = 0.0001\n",
    "    W0 = lambda: 2 * random.rand(Ys_tr.shape[0], Xs_tr.shape[0]) - 1\n",
    "    # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "    #                   Hyperparams\n",
    "    # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "    # SGD\n",
    "    sgd = lambda: sgd_minibatch(\n",
    "        Xs_tr,\n",
    "        Ys_tr,\n",
    "        gamma=gamma,\n",
    "        W0=W0(),\n",
    "        alpha=alpha,\n",
    "        B=batch,\n",
    "        num_epochs=10,\n",
    "        monitor_period=monitor_period,\n",
    "    )\n",
    "    paramHistSGD, secs = timeit(sgd)\n",
    "    errorsSGD = [\n",
    "        multinomial_logreg_error(Xs_tr, Ys_tr, param) for param in paramHistSGD\n",
    "    ]\n",
    "    for i, j in enumerate(errorsSGD):\n",
    "        print(f\"Accuracy Measurement {i+1}: {1-j}\")\n",
    "    print(f\"SGD Normal B={batch}, A={alpha}, gamma={gamma} Took {secs} Seconds\\n\")\n",
    "\n",
    "    # SGD Sequential\n",
    "    params = 2 * random.rand(Ys_tr.shape[0], Xs_tr.shape[0]) - 1\n",
    "    sgds = lambda: sgd_minibatch_sequential_scan(\n",
    "        Xs_tr,\n",
    "        Ys_tr,\n",
    "        gamma=gamma,\n",
    "        W0=W0(),\n",
    "        alpha=alpha,\n",
    "        B=batch,\n",
    "        num_epochs=10,\n",
    "        monitor_period=monitor_period,\n",
    "    )\n",
    "    paramHistSGDS, secs = timeit(sgds)\n",
    "    errorsSGDS = [\n",
    "        multinomial_logreg_error(Xs_tr, Ys_tr, param) for param in paramHistSGDS\n",
    "    ]\n",
    "\n",
    "    for i, j in enumerate(errorsSGDS):\n",
    "        print(f\"Accuracy Measurement {i+1}: {1-j}\")\n",
    "    print(f\"SGD Sequential B={batch}, A={alpha}, gamma={gamma} Took {secs} Seconds\\n\")\n",
    "\n",
    "    # SGD Random Shuffling\n",
    "    params = 2 * random.rand(Ys_tr.shape[0], Xs_tr.shape[0]) - 1\n",
    "    sgdrs = lambda: sgd_minibatch_random_reshuffling(\n",
    "        Xs_tr,\n",
    "        Ys_tr,\n",
    "        gamma=gamma,\n",
    "        W0=W0(),\n",
    "        alpha=alpha,\n",
    "        B=batch,\n",
    "        num_epochs=10,\n",
    "        monitor_period=monitor_period,\n",
    "    )\n",
    "    paramHistSGDRS, secs = timeit(sgdrs)\n",
    "    errorsSGDRS = [\n",
    "        multinomial_logreg_error(Xs_tr, Ys_tr, param) for param in paramHistSGDRS\n",
    "    ]\n",
    "    for i, j in enumerate(errorsSGDRS):\n",
    "        print(f\"Accuracy Measurement {i+1}: {1-j}\")\n",
    "    print(\n",
    "        f\"SGD Random Shuffling B={batch}, A={alpha}, gamma={gamma} Took {secs} Seconds\\n\"\n",
    "    )\n",
    "    # GRAPHING\n",
    "    # gradient descent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRAPHING\n",
    "# gradient descent\n",
    "trLossGD = [\n",
    "    multinomial_logreg_batch_loss(Xs_tr, Ys_tr, 0.0001, weight)\n",
    "    for weight in paramHist\n",
    "]\n",
    "trErrorGD = [multinomial_logreg_error(Xs_tr, Ys_tr, weight) for weight in paramHist]\n",
    "teLossGD = [\n",
    "    multinomial_logreg_batch_loss(Xs_te, Ys_te, 0.0001, weight)\n",
    "    for weight in paramHist\n",
    "]\n",
    "teErrorGD = [multinomial_logreg_error(Xs_te, Ys_te, weight) for weight in paramHist]\n",
    "iters = [(10 * i + 1) for i in range(len(paramHist))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "iters = [(10 * i + 1) for i in range(len(paramHistSGD))]\n",
    "trErrorSGD = [multinomial_logreg_error(Xs_tr, Ys_tr, weight) for weight in paramHistSGD]\n",
    "teErrorSGD = [multinomial_logreg_error(Xs_te, Ys_te, weight) for weight in paramHistSGD]\n",
    "\n",
    "trErrorSGDS = [multinomial_logreg_error(Xs_tr, Ys_tr, weight) for weight in paramHistSGDS]\n",
    "teErrorSGDS = [multinomial_logreg_error(Xs_te, Ys_te, weight) for weight in paramHistSGDS]\n",
    "\n",
    "trErrorSGDRS = [multinomial_logreg_error(Xs_tr, Ys_tr, weight) for weight in paramHistSGDRS]\n",
    "teErrorSGDRS = [multinomial_logreg_error(Xs_te, Ys_te, weight) for weight in paramHistSGDRS]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, axs = pyplot.subplots(2,1)\n",
    "axs[1].plot(iters, trErrorSGDRS)\n",
    "axs[1].set_title(\"Testing Error\")\n",
    "axs[0].plot(iters, trErrorSGDRS, \"tab:green\")\n",
    "axs[0].set_title(\"Training Error\")\n",
    "# axs[1, 0].plot(iters, trLossGD)\n",
    "# axs[1, 0].set_title(\"Training Loss\")\n",
    "# axs[0, 0].plot(iters, trErrorGD, \"tab:orange\")\n",
    "# axs[0, 0].set_title(\"Training Error\")\n",
    "# axs[1, 1].plot(iters, teLossGD, \"tab:green\")\n",
    "# axs[1, 1].set_title(\"Test Loss\")\n",
    "# axs[0, 1].plot(iters, teErrorGD, \"tab:red\")\n",
    "# axs[0, 1].set_title(\"Test Error\")\n",
    "\n",
    "for ax in axs:\n",
    "    ax.set(xlabel=\"Iteration\", ylabel=\"Error\")\n",
    "# for ax in axs[1]:\n",
    "#     ax.set(xlabel=\"Iteration\", ylabel=\"Loss\")\n",
    "fig.suptitle('Error/Loss for Stochastic Gradient Descent Random Shuffling B = 1') \n",
    "# Hide x labels and tick labels for top plots and y ticks for right plots.\n",
    "for ax in axs.flat:\n",
    "    ax.label_outer()\n",
    "pyplot.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
