{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Measurement 1: 0.8041\n",
      "Accuracy Measurement 2: 0.4698833333333333\n",
      "Accuracy Measurement 3: 0.36944999999999995\n",
      "Accuracy Measurement 4: 0.33973333333333333\n",
      "Accuracy Measurement 5: 0.32991666666666664\n",
      "Accuracy Measurement 6: 0.3261166666666667\n",
      "Accuracy Measurement 7: 0.32415000000000005\n",
      "Accuracy Measurement 8: 0.32335\n",
      "Accuracy Measurement 9: 0.3425166666666667\n",
      "Accuracy Measurement 10: 0.4485\n",
      "Accuracy Measurement 11: 0.5864666666666667\n",
      "Accuracy Measurement 12: 0.7004666666666667\n",
      "Accuracy Measurement 13: 0.7767833333333334\n",
      "Accuracy Measurement 14: 0.8264666666666667\n",
      "Accuracy Measurement 15: 0.8568833333333333\n",
      "Accuracy Measurement 16: 0.8743833333333333\n",
      "Accuracy Measurement 17: 0.8851333333333333\n",
      "Accuracy Measurement 18: 0.8909833333333333\n",
      "Accuracy Measurement 19: 0.8947166666666666\n",
      "Accuracy Measurement 20: 0.8972\n",
      "Accuracy Measurement 21: 0.8987499999999999\n",
      "Accuracy Measurement 22: 0.8996666666666666\n",
      "Accuracy Measurement 23: 0.90025\n",
      "Accuracy Measurement 24: 0.9005\n",
      "Accuracy Measurement 25: 0.9007833333333334\n",
      "Accuracy Measurement 26: 0.9009166666666667\n",
      "Accuracy Measurement 27: 0.901\n",
      "Accuracy Measurement 28: 0.90105\n",
      "Accuracy Measurement 29: 0.9011166666666667\n",
      "Accuracy Measurement 30: 0.9011833333333333\n",
      "Accuracy Measurement 31: 0.9012166666666667\n",
      "Accuracy Measurement 32: 0.90125\n",
      "Accuracy Measurement 33: 0.9012666666666667\n",
      "Accuracy Measurement 34: 0.9012833333333333\n",
      "Accuracy Measurement 35: 0.9012833333333333\n",
      "Accuracy Measurement 36: 0.9012833333333333\n",
      "Accuracy Measurement 37: 0.9012833333333333\n",
      "Accuracy Measurement 38: 0.9012833333333333\n",
      "Accuracy Measurement 39: 0.9012833333333333\n",
      "Accuracy Measurement 40: 0.9012833333333333\n",
      "Accuracy Measurement 41: 0.9012833333333333\n",
      "Accuracy Measurement 42: 0.9012833333333333\n",
      "Accuracy Measurement 43: 0.9012833333333333\n",
      "Accuracy Measurement 44: 0.9012833333333333\n",
      "Accuracy Measurement 45: 0.9012833333333333\n",
      "Accuracy Measurement 46: 0.9012833333333333\n",
      "Accuracy Measurement 47: 0.9012833333333333\n",
      "Accuracy Measurement 48: 0.9012833333333333\n",
      "Accuracy Measurement 49: 0.9012833333333333\n",
      "Accuracy Measurement 50: 0.9012833333333333\n",
      "Accuracy Measurement 51: 0.9012833333333333\n",
      "Accuracy Measurement 52: 0.9012833333333333\n",
      "Accuracy Measurement 53: 0.9012833333333333\n",
      "Accuracy Measurement 54: 0.9012833333333333\n",
      "Accuracy Measurement 55: 0.9012833333333333\n",
      "Accuracy Measurement 56: 0.9012833333333333\n",
      "Accuracy Measurement 57: 0.9012833333333333\n",
      "Accuracy Measurement 58: 0.9012833333333333\n",
      "Accuracy Measurement 59: 0.9012833333333333\n",
      "Accuracy Measurement 60: 0.9012833333333333\n",
      "Accuracy Measurement 61: 0.9012833333333333\n",
      "Accuracy Measurement 62: 0.9012833333333333\n",
      "Accuracy Measurement 63: 0.9012833333333333\n",
      "Accuracy Measurement 64: 0.9012833333333333\n",
      "Accuracy Measurement 65: 0.9012833333333333\n",
      "Accuracy Measurement 66: 0.9012833333333333\n",
      "Accuracy Measurement 67: 0.9012833333333333\n",
      "Accuracy Measurement 68: 0.9012833333333333\n",
      "Accuracy Measurement 69: 0.9012833333333333\n",
      "Accuracy Measurement 70: 0.9012833333333333\n",
      "Accuracy Measurement 71: 0.9012833333333333\n",
      "Accuracy Measurement 72: 0.9012833333333333\n",
      "Accuracy Measurement 73: 0.9012833333333333\n",
      "Accuracy Measurement 74: 0.9012833333333333\n",
      "Accuracy Measurement 75: 0.9012833333333333\n",
      "Accuracy Measurement 76: 0.9012833333333333\n",
      "Accuracy Measurement 77: 0.9012833333333333\n",
      "Accuracy Measurement 78: 0.9012833333333333\n",
      "Accuracy Measurement 79: 0.9012833333333333\n",
      "Accuracy Measurement 80: 0.9012833333333333\n",
      "Accuracy Measurement 81: 0.9012833333333333\n",
      "Accuracy Measurement 82: 0.9012833333333333\n",
      "Accuracy Measurement 83: 0.9012833333333333\n",
      "Accuracy Measurement 84: 0.9012833333333333\n",
      "Accuracy Measurement 85: 0.9012833333333333\n",
      "Accuracy Measurement 86: 0.9012833333333333\n",
      "Accuracy Measurement 87: 0.9012833333333333\n",
      "Accuracy Measurement 88: 0.9012833333333333\n",
      "Accuracy Measurement 89: 0.9012833333333333\n",
      "Accuracy Measurement 90: 0.9012833333333333\n",
      "Accuracy Measurement 91: 0.9012833333333333\n",
      "Accuracy Measurement 92: 0.9012833333333333\n",
      "Accuracy Measurement 93: 0.9012833333333333\n",
      "Accuracy Measurement 94: 0.9012833333333333\n",
      "Accuracy Measurement 95: 0.9012833333333333\n",
      "Accuracy Measurement 96: 0.9012833333333333\n",
      "Accuracy Measurement 97: 0.9012833333333333\n",
      "Accuracy Measurement 98: 0.9012833333333333\n",
      "Accuracy Measurement 99: 0.9012833333333333\n",
      "Accuracy Measurement 100: 0.9012833333333333\n",
      "Accuracy Measurement 101: 0.9012833333333333\n",
      "Gradient Descent (1000 Iters) Took 93.92346906661987 Seconds\n",
      "Accuracy Measurement 1: 0.9033833333333333\n",
      "Accuracy Measurement 2: 0.6496\n",
      "Accuracy Measurement 3: 0.4861833333333333\n",
      "Accuracy Measurement 4: 0.3998166666666667\n",
      "Accuracy Measurement 5: 0.36888333333333334\n",
      "Accuracy Measurement 6: 0.3483166666666667\n",
      "Accuracy Measurement 7: 0.3290666666666666\n",
      "Accuracy Measurement 8: 0.3240166666666666\n",
      "Accuracy Measurement 9: 0.31828333333333336\n",
      "Accuracy Measurement 10: 0.31115000000000004\n",
      "Accuracy Measurement 11: 0.3080333333333334\n",
      "Accuracy Measurement 12: 0.3101666666666667\n",
      "Accuracy Measurement 13: 0.30500000000000005\n",
      "Accuracy Measurement 14: 0.30189999999999995\n",
      "Accuracy Measurement 15: 0.2997666666666666\n",
      "Accuracy Measurement 16: 0.29703333333333337\n",
      "Accuracy Measurement 17: 0.2945333333333333\n",
      "Accuracy Measurement 18: 0.2936333333333333\n",
      "Accuracy Measurement 19: 0.29333333333333333\n",
      "Accuracy Measurement 20: 0.30615000000000003\n",
      "Accuracy Measurement 21: 0.32796666666666663\n",
      "Accuracy Measurement 22: 0.3671333333333333\n",
      "Accuracy Measurement 23: 0.4124833333333333\n",
      "Accuracy Measurement 24: 0.4624166666666667\n",
      "Accuracy Measurement 25: 0.5115000000000001\n",
      "Accuracy Measurement 26: 0.5668166666666667\n",
      "Accuracy Measurement 27: 0.6161\n",
      "Accuracy Measurement 28: 0.6549333333333334\n",
      "Accuracy Measurement 29: 0.69215\n",
      "Accuracy Measurement 30: 0.72155\n",
      "Accuracy Measurement 31: 0.7423500000000001\n",
      "Accuracy Measurement 32: 0.7653333333333333\n",
      "Accuracy Measurement 33: 0.7863333333333333\n",
      "Accuracy Measurement 34: 0.8023\n",
      "Accuracy Measurement 35: 0.8179666666666667\n",
      "Accuracy Measurement 36: 0.8293333333333334\n",
      "Accuracy Measurement 37: 0.8387\n",
      "Accuracy Measurement 38: 0.8468166666666667\n",
      "Accuracy Measurement 39: 0.8536833333333333\n",
      "Accuracy Measurement 40: 0.86025\n",
      "Accuracy Measurement 41: 0.8646333333333334\n",
      "Accuracy Measurement 42: 0.8694666666666666\n",
      "Accuracy Measurement 43: 0.8737166666666667\n",
      "Accuracy Measurement 44: 0.8777\n",
      "Accuracy Measurement 45: 0.8814333333333333\n",
      "Accuracy Measurement 46: 0.8839833333333333\n",
      "Accuracy Measurement 47: 0.8860833333333333\n",
      "Accuracy Measurement 48: 0.8868666666666667\n",
      "Accuracy Measurement 49: 0.8883166666666666\n",
      "Accuracy Measurement 50: 0.8892166666666667\n",
      "Accuracy Measurement 51: 0.8906499999999999\n",
      "Accuracy Measurement 52: 0.89145\n",
      "Accuracy Measurement 53: 0.8925\n",
      "Accuracy Measurement 54: 0.89355\n",
      "Accuracy Measurement 55: 0.8941666666666667\n",
      "Accuracy Measurement 56: 0.8951666666666667\n",
      "Accuracy Measurement 57: 0.8956833333333334\n",
      "Accuracy Measurement 58: 0.89635\n",
      "Accuracy Measurement 59: 0.8967333333333334\n",
      "Accuracy Measurement 60: 0.89715\n",
      "Accuracy Measurement 61: 0.8977166666666667\n",
      "Accuracy Measurement 62: 0.8982666666666667\n",
      "Accuracy Measurement 63: 0.8986333333333334\n",
      "Accuracy Measurement 64: 0.8989166666666667\n",
      "Accuracy Measurement 65: 0.8991666666666667\n",
      "Accuracy Measurement 66: 0.8993833333333333\n",
      "Accuracy Measurement 67: 0.89955\n",
      "Accuracy Measurement 68: 0.8996666666666666\n",
      "Accuracy Measurement 69: 0.8998833333333334\n",
      "Accuracy Measurement 70: 0.90005\n",
      "Accuracy Measurement 71: 0.9002\n",
      "Accuracy Measurement 72: 0.90025\n",
      "Accuracy Measurement 73: 0.9003166666666667\n",
      "Accuracy Measurement 74: 0.9003833333333333\n",
      "Accuracy Measurement 75: 0.9004333333333333\n",
      "Accuracy Measurement 76: 0.9005166666666666\n",
      "Accuracy Measurement 77: 0.90055\n",
      "Accuracy Measurement 78: 0.9005666666666666\n",
      "Accuracy Measurement 79: 0.9006\n",
      "Accuracy Measurement 80: 0.9006333333333333\n",
      "Accuracy Measurement 81: 0.9006666666666667\n",
      "Accuracy Measurement 82: 0.9007333333333334\n",
      "Accuracy Measurement 83: 0.9008\n",
      "Accuracy Measurement 84: 0.9008166666666667\n",
      "Accuracy Measurement 85: 0.9008333333333334\n",
      "Accuracy Measurement 86: 0.9008833333333334\n",
      "Accuracy Measurement 87: 0.9009\n",
      "Accuracy Measurement 88: 0.9009166666666667\n",
      "Accuracy Measurement 89: 0.9009166666666667\n",
      "Accuracy Measurement 90: 0.9009166666666667\n",
      "Accuracy Measurement 91: 0.9009333333333334\n",
      "Accuracy Measurement 92: 0.901\n",
      "Accuracy Measurement 93: 0.9010166666666667\n",
      "Accuracy Measurement 94: 0.9010166666666667\n",
      "Accuracy Measurement 95: 0.9010166666666667\n",
      "Accuracy Measurement 96: 0.9010166666666667\n",
      "Accuracy Measurement 97: 0.9010333333333334\n",
      "Accuracy Measurement 98: 0.90105\n",
      "Accuracy Measurement 99: 0.90105\n",
      "Accuracy Measurement 100: 0.90105\n",
      "Accuracy Measurement 101: 0.90105\n",
      "SGD Normal B=60, A=0.05, gamma=0.0001 Took 12.033759355545044 Seconds\n",
      "\n",
      "Accuracy Measurement 1: 0.8812166666666666\n",
      "Accuracy Measurement 2: 0.6450166666666667\n",
      "Accuracy Measurement 3: 0.4618833333333333\n",
      "Accuracy Measurement 4: 0.3977166666666667\n",
      "Accuracy Measurement 5: 0.34148333333333336\n",
      "Accuracy Measurement 6: 0.3154666666666667\n",
      "Accuracy Measurement 7: 0.3051666666666667\n",
      "Accuracy Measurement 8: 0.29356666666666664\n",
      "Accuracy Measurement 9: 0.2918666666666667\n",
      "Accuracy Measurement 10: 0.28941666666666666\n",
      "Accuracy Measurement 11: 0.2884\n",
      "Accuracy Measurement 12: 0.2806166666666666\n",
      "Accuracy Measurement 13: 0.28003333333333336\n",
      "Accuracy Measurement 14: 0.2816833333333333\n",
      "Accuracy Measurement 15: 0.27888333333333337\n",
      "Accuracy Measurement 16: 0.2781166666666667\n",
      "Accuracy Measurement 17: 0.2782\n",
      "Accuracy Measurement 18: 0.2771\n",
      "Accuracy Measurement 19: 0.2811\n",
      "Accuracy Measurement 20: 0.2949333333333334\n",
      "Accuracy Measurement 21: 0.32266666666666666\n",
      "Accuracy Measurement 22: 0.3613166666666666\n",
      "Accuracy Measurement 23: 0.41533333333333333\n",
      "Accuracy Measurement 24: 0.4671333333333333\n",
      "Accuracy Measurement 25: 0.5220666666666667\n",
      "Accuracy Measurement 26: 0.5709500000000001\n",
      "Accuracy Measurement 27: 0.6127333333333334\n",
      "Accuracy Measurement 28: 0.6536666666666666\n",
      "Accuracy Measurement 29: 0.6872\n",
      "Accuracy Measurement 30: 0.7187833333333333\n",
      "Accuracy Measurement 31: 0.74275\n",
      "Accuracy Measurement 32: 0.7647333333333333\n",
      "Accuracy Measurement 33: 0.7871166666666667\n",
      "Accuracy Measurement 34: 0.8026333333333333\n",
      "Accuracy Measurement 35: 0.8172666666666667\n",
      "Accuracy Measurement 36: 0.8281666666666667\n",
      "Accuracy Measurement 37: 0.8374666666666667\n",
      "Accuracy Measurement 38: 0.8464\n",
      "Accuracy Measurement 39: 0.85355\n",
      "Accuracy Measurement 40: 0.8600833333333333\n",
      "Accuracy Measurement 41: 0.8646\n",
      "Accuracy Measurement 42: 0.8693500000000001\n",
      "Accuracy Measurement 43: 0.8742166666666666\n",
      "Accuracy Measurement 44: 0.8773166666666666\n",
      "Accuracy Measurement 45: 0.8807\n",
      "Accuracy Measurement 46: 0.883\n",
      "Accuracy Measurement 47: 0.8848666666666667\n",
      "Accuracy Measurement 48: 0.8866\n",
      "Accuracy Measurement 49: 0.8879166666666667\n",
      "Accuracy Measurement 50: 0.8894166666666666\n",
      "Accuracy Measurement 51: 0.8905\n",
      "Accuracy Measurement 52: 0.8914666666666666\n",
      "Accuracy Measurement 53: 0.8926333333333334\n",
      "Accuracy Measurement 54: 0.8934833333333333\n",
      "Accuracy Measurement 55: 0.8945666666666666\n",
      "Accuracy Measurement 56: 0.8952666666666667\n",
      "Accuracy Measurement 57: 0.8957666666666667\n",
      "Accuracy Measurement 58: 0.8962333333333333\n",
      "Accuracy Measurement 59: 0.8966166666666666\n",
      "Accuracy Measurement 60: 0.8970333333333333\n",
      "Accuracy Measurement 61: 0.8973\n",
      "Accuracy Measurement 62: 0.8978166666666667\n",
      "Accuracy Measurement 63: 0.8983166666666667\n",
      "Accuracy Measurement 64: 0.8986166666666666\n",
      "Accuracy Measurement 65: 0.8988\n",
      "Accuracy Measurement 66: 0.8991166666666667\n",
      "Accuracy Measurement 67: 0.89925\n",
      "Accuracy Measurement 68: 0.89945\n",
      "Accuracy Measurement 69: 0.8995166666666666\n",
      "Accuracy Measurement 70: 0.8996666666666666\n",
      "Accuracy Measurement 71: 0.89975\n",
      "Accuracy Measurement 72: 0.8999333333333334\n",
      "Accuracy Measurement 73: 0.9001166666666667\n",
      "Accuracy Measurement 74: 0.9002333333333333\n",
      "Accuracy Measurement 75: 0.9002666666666667\n",
      "Accuracy Measurement 76: 0.9003666666666666\n",
      "Accuracy Measurement 77: 0.9004\n",
      "Accuracy Measurement 78: 0.9004833333333333\n",
      "Accuracy Measurement 79: 0.9005\n",
      "Accuracy Measurement 80: 0.90055\n",
      "Accuracy Measurement 81: 0.9005666666666666\n",
      "Accuracy Measurement 82: 0.9005666666666666\n",
      "Accuracy Measurement 83: 0.9006666666666667\n",
      "Accuracy Measurement 84: 0.9007499999999999\n",
      "Accuracy Measurement 85: 0.9008\n",
      "Accuracy Measurement 86: 0.9008166666666667\n",
      "Accuracy Measurement 87: 0.9008666666666667\n",
      "Accuracy Measurement 88: 0.9009\n",
      "Accuracy Measurement 89: 0.9009\n",
      "Accuracy Measurement 90: 0.9009\n",
      "Accuracy Measurement 91: 0.9009166666666667\n",
      "Accuracy Measurement 92: 0.9009166666666667\n",
      "Accuracy Measurement 93: 0.90095\n",
      "Accuracy Measurement 94: 0.90095\n",
      "Accuracy Measurement 95: 0.901\n",
      "Accuracy Measurement 96: 0.9010166666666667\n",
      "Accuracy Measurement 97: 0.9010166666666667\n",
      "Accuracy Measurement 98: 0.9010166666666667\n",
      "Accuracy Measurement 99: 0.9010166666666667\n",
      "Accuracy Measurement 100: 0.9010333333333334\n",
      "Accuracy Measurement 101: 0.90105\n",
      "SGD Sequential B=60, A=0.05, gamma=0.0001 Took 9.344888687133789 Seconds\n",
      "\n",
      "Accuracy Measurement 1: 0.8937333333333334\n",
      "Accuracy Measurement 2: 0.6674\n",
      "Accuracy Measurement 3: 0.5137666666666667\n",
      "Accuracy Measurement 4: 0.39470000000000005\n",
      "Accuracy Measurement 5: 0.3732833333333333\n",
      "Accuracy Measurement 6: 0.3327833333333333\n",
      "Accuracy Measurement 7: 0.3099333333333333\n",
      "Accuracy Measurement 8: 0.2967666666666666\n",
      "Accuracy Measurement 9: 0.28526666666666667\n",
      "Accuracy Measurement 10: 0.2852\n",
      "Accuracy Measurement 11: 0.27831666666666666\n",
      "Accuracy Measurement 12: 0.2771666666666667\n",
      "Accuracy Measurement 13: 0.27225\n",
      "Accuracy Measurement 14: 0.27203333333333335\n",
      "Accuracy Measurement 15: 0.2715666666666666\n",
      "Accuracy Measurement 16: 0.2718666666666667\n",
      "Accuracy Measurement 17: 0.2719666666666667\n",
      "Accuracy Measurement 18: 0.27193333333333336\n",
      "Accuracy Measurement 19: 0.27395\n",
      "Accuracy Measurement 20: 0.2853333333333333\n",
      "Accuracy Measurement 21: 0.31518333333333337\n",
      "Accuracy Measurement 22: 0.3582666666666666\n",
      "Accuracy Measurement 23: 0.40385000000000004\n",
      "Accuracy Measurement 24: 0.46543333333333337\n",
      "Accuracy Measurement 25: 0.5146333333333333\n",
      "Accuracy Measurement 26: 0.5709166666666667\n",
      "Accuracy Measurement 27: 0.6165499999999999\n",
      "Accuracy Measurement 28: 0.6526333333333334\n",
      "Accuracy Measurement 29: 0.68465\n",
      "Accuracy Measurement 30: 0.7155166666666667\n",
      "Accuracy Measurement 31: 0.7440666666666667\n",
      "Accuracy Measurement 32: 0.7653333333333333\n",
      "Accuracy Measurement 33: 0.7834666666666666\n",
      "Accuracy Measurement 34: 0.8004166666666667\n",
      "Accuracy Measurement 35: 0.8155666666666667\n",
      "Accuracy Measurement 36: 0.8242833333333334\n",
      "Accuracy Measurement 37: 0.8365833333333333\n",
      "Accuracy Measurement 38: 0.8461333333333333\n",
      "Accuracy Measurement 39: 0.8546666666666667\n",
      "Accuracy Measurement 40: 0.86125\n",
      "Accuracy Measurement 41: 0.8652166666666666\n",
      "Accuracy Measurement 42: 0.8691333333333333\n",
      "Accuracy Measurement 43: 0.87265\n",
      "Accuracy Measurement 44: 0.8764166666666666\n",
      "Accuracy Measurement 45: 0.87915\n",
      "Accuracy Measurement 46: 0.88245\n",
      "Accuracy Measurement 47: 0.8844333333333333\n",
      "Accuracy Measurement 48: 0.8860833333333333\n",
      "Accuracy Measurement 49: 0.88825\n",
      "Accuracy Measurement 50: 0.8895333333333333\n",
      "Accuracy Measurement 51: 0.8908166666666667\n",
      "Accuracy Measurement 52: 0.8917166666666667\n",
      "Accuracy Measurement 53: 0.8928\n",
      "Accuracy Measurement 54: 0.8938\n",
      "Accuracy Measurement 55: 0.8947333333333334\n",
      "Accuracy Measurement 56: 0.8951833333333333\n",
      "Accuracy Measurement 57: 0.8956\n",
      "Accuracy Measurement 58: 0.8961333333333333\n",
      "Accuracy Measurement 59: 0.8966833333333333\n",
      "Accuracy Measurement 60: 0.89705\n",
      "Accuracy Measurement 61: 0.8975666666666666\n",
      "Accuracy Measurement 62: 0.8981166666666667\n",
      "Accuracy Measurement 63: 0.8984166666666666\n",
      "Accuracy Measurement 64: 0.8987\n",
      "Accuracy Measurement 65: 0.8989166666666667\n",
      "Accuracy Measurement 66: 0.8991166666666667\n",
      "Accuracy Measurement 67: 0.8993333333333333\n",
      "Accuracy Measurement 68: 0.8994\n",
      "Accuracy Measurement 69: 0.8995166666666666\n",
      "Accuracy Measurement 70: 0.8996\n",
      "Accuracy Measurement 71: 0.8998333333333334\n",
      "Accuracy Measurement 72: 0.90005\n",
      "Accuracy Measurement 73: 0.90015\n",
      "Accuracy Measurement 74: 0.9002666666666667\n",
      "Accuracy Measurement 75: 0.9003333333333333\n",
      "Accuracy Measurement 76: 0.9003333333333333\n",
      "Accuracy Measurement 77: 0.9004\n",
      "Accuracy Measurement 78: 0.9005\n",
      "Accuracy Measurement 79: 0.90055\n",
      "Accuracy Measurement 80: 0.9005666666666666\n",
      "Accuracy Measurement 81: 0.9005666666666666\n",
      "Accuracy Measurement 82: 0.9006333333333333\n",
      "Accuracy Measurement 83: 0.9006833333333333\n",
      "Accuracy Measurement 84: 0.9007333333333334\n",
      "Accuracy Measurement 85: 0.9008166666666667\n",
      "Accuracy Measurement 86: 0.9008166666666667\n",
      "Accuracy Measurement 87: 0.9008666666666667\n",
      "Accuracy Measurement 88: 0.9009\n",
      "Accuracy Measurement 89: 0.9009\n",
      "Accuracy Measurement 90: 0.9009166666666667\n",
      "Accuracy Measurement 91: 0.9009333333333334\n",
      "Accuracy Measurement 92: 0.90095\n",
      "Accuracy Measurement 93: 0.9009666666666667\n",
      "Accuracy Measurement 94: 0.901\n",
      "Accuracy Measurement 95: 0.9010166666666667\n",
      "Accuracy Measurement 96: 0.9010166666666667\n",
      "Accuracy Measurement 97: 0.9010166666666667\n",
      "Accuracy Measurement 98: 0.9010166666666667\n",
      "Accuracy Measurement 99: 0.9010333333333334\n",
      "Accuracy Measurement 100: 0.90105\n",
      "Accuracy Measurement 101: 0.90105\n",
      "SGD Random Shuffling B=60, A=0.05, gamma=0.0001 Took 11.419187307357788 Seconds\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy\n",
    "from numpy import random\n",
    "import scipy\n",
    "from scipy.special import softmax\n",
    "import mnist\n",
    "import pickle\n",
    "\n",
    "# you can use matplotlib for plotting\n",
    "from time import time\n",
    "import matplotlib\n",
    "from matplotlib import pyplot\n",
    "\n",
    "mnist_data_directory = os.path.join(os.path.abspath(''), \"data\")\n",
    "\n",
    "# TODO add any additional imports and global variables\n",
    "\n",
    "\n",
    "def load_MNIST_dataset():\n",
    "    PICKLE_FILE = os.path.join(mnist_data_directory, \"MNIST.pickle\")\n",
    "    try:\n",
    "        dataset = pickle.load(open(PICKLE_FILE, \"rb\"))\n",
    "    except:\n",
    "        # load the MNIST dataset\n",
    "        mnist_data = mnist.MNIST(mnist_data_directory, return_type=\"numpy\", gz=True)\n",
    "        Xs_tr, Lbls_tr = mnist_data.load_training()\n",
    "        Xs_tr = Xs_tr.transpose() / 255.0\n",
    "        Ys_tr = numpy.zeros((10, 60000))\n",
    "        for i in range(60000):\n",
    "            Ys_tr[Lbls_tr[i], i] = 1.0  # one-hot encode each label\n",
    "        Xs_tr = numpy.ascontiguousarray(Xs_tr)\n",
    "        Ys_tr = numpy.ascontiguousarray(Ys_tr)\n",
    "        Xs_te, Lbls_te = mnist_data.load_testing()\n",
    "        Xs_te = Xs_te.transpose() / 255.0\n",
    "        Ys_te = numpy.zeros((10, 10000))\n",
    "        for i in range(10000):\n",
    "            Ys_te[Lbls_te[i], i] = 1.0  # one-hot encode each label\n",
    "        Xs_te = numpy.ascontiguousarray(Xs_te)\n",
    "        Ys_te = numpy.ascontiguousarray(Ys_te)\n",
    "        dataset = (Xs_tr, Ys_tr, Xs_te, Ys_te)\n",
    "        pickle.dump(dataset, open(PICKLE_FILE, \"wb\"))\n",
    "    return dataset\n",
    "\n",
    "\n",
    "# compute the cross-entropy loss of the classifier\n",
    "#\n",
    "# x         examples          (d)\n",
    "# y         labels            (c)\n",
    "# gamma     L2 regularization constant\n",
    "# W         parameters        (c * d)\n",
    "#\n",
    "# returns   the model cross-entropy loss\n",
    "def multinomial_logreg_loss_i(x, y, gamma, W):\n",
    "    return numpy.dot(y, -numpy.log(softmax(W @ x)))\n",
    "\n",
    "\n",
    "# compute the gradient of a single example of the multinomial logistic regression objective, with regularization\n",
    "#\n",
    "# x         training example   (d)\n",
    "# y         training label     (c)\n",
    "# gamma     L2 regularization constant\n",
    "# W         parameters        (c * d)\n",
    "#\n",
    "# returns   the gradient of the loss with respect to the model parameters W\n",
    "def multinomial_logreg_grad_i(x, y, gamma, W):\n",
    "    return numpy.outer(softmax(W @ x) - y, x) + gamma * W\n",
    "\n",
    "\n",
    "# compute the error of the classifier\n",
    "#\n",
    "# Xs        examples          (d * n)\n",
    "# Ys        labels            (c * n)\n",
    "# W         parameters        (c * d)\n",
    "#\n",
    "# returns   the model error as a percentage of incorrect labels\n",
    "def multinomial_logreg_error(Xs, Ys, W):\n",
    "    val = softmax(W @ Xs)\n",
    "    preds = numpy.zeros_like(val)\n",
    "    preds[numpy.argmax(val, axis=0), range(val.shape[1])] = 1\n",
    "    return numpy.sum(preds * Ys) / Ys.shape[1]\n",
    "\n",
    "    # TODO students should implement this\n",
    "\n",
    "\n",
    "# compute the gradient of the multinomial logistic regression objective on a batch, with regularization\n",
    "#\n",
    "# Xs        training examples (d * n)\n",
    "# Ys        training labels   (c * n)\n",
    "# gamma     L2 regularization constant\n",
    "# W         parameters        (c * d)\n",
    "# ii        indices of the batch (an iterable or range)\n",
    "#\n",
    "# returns   the gradient of the model parameters\n",
    "def multinomial_logreg_batch_grad(Xs, Ys, gamma, W, ii=None):\n",
    "    if ii is None:\n",
    "        ii = range(Xs.shape[1])\n",
    "        X = Xs\n",
    "        Y = Ys\n",
    "    else:\n",
    "        X = Xs[:, ii]\n",
    "        Y = Ys[:, ii]\n",
    "    # a starter solution using an average of the example gradients\n",
    "    (d, n) = Xs.shape\n",
    "    return ((softmax(W @ X) - Y) @ X.T) / len(ii)\n",
    "\n",
    "    # acc = W * 0.0\n",
    "    # for i in ii:\n",
    "    #     acc += multinomial_logreg_grad_i(Xs[:, i], Ys[:, i], gamma, W)\n",
    "    # return acc / len(ii)\n",
    "\n",
    "\n",
    "# compute the cross-entropy loss of the classifier on a batch, with regularization\n",
    "#\n",
    "# Xs        examples          (d * n)\n",
    "# Ys        labels            (c * n)\n",
    "# gamma     L2 regularization constant\n",
    "# W         parameters        (c * d)\n",
    "# ii        indices of the batch (an iterable or range)\n",
    "#\n",
    "# returns   the model cross-entropy loss\n",
    "def multinomial_logreg_batch_loss(Xs, Ys, gamma, W, ii=None):\n",
    "    if ii is None:\n",
    "        ii = range(Xs.shape[1])\n",
    "        X = Xs\n",
    "        Y = Ys\n",
    "    else:\n",
    "        X = Xs[:, ii]\n",
    "        Y = Ys[:, ii]\n",
    "\n",
    "    (d, n) = Xs.shape\n",
    "    frobNorm = numpy.linalg.norm(W) ** 2\n",
    "    return (\n",
    "        numpy.sum(Y * (-numpy.log(softmax(W @ X, axis=0)))) / len(ii)\n",
    "        + gamma * frobNorm / 2\n",
    "    )\n",
    "\n",
    "    # acc = 0.0\n",
    "    # for i in ii:\n",
    "    #     acc += multinomial_logreg_loss_i(Xs[:, i], Ys[:, i], gamma, W)\n",
    "    # return acc / len(ii)\n",
    "\n",
    "\n",
    "# run gradient descent on a multinomial logistic regression objective, with regularization\n",
    "#\n",
    "# Xs            training examples (d * n)\n",
    "# Ys            training labels   (d * c)\n",
    "# gamma         L2 regularization constant\n",
    "# W0            the initial value of the parameters (c * d)\n",
    "# alpha         step size/learning rate\n",
    "# num_iters     number of iterations to run\n",
    "# monitor_freq  how frequently to output the parameter vector\n",
    "#\n",
    "# returns       a list of models parameters, one every \"monitor_freq\" iterations\n",
    "#               should return model parameters before iteration 0, iteration monitor_freq, iteration 2*monitor_freq, and again at the end\n",
    "#               for a total of (num_iters/monitor_freq)+1 models, if num_iters is divisible by monitor_freq.\n",
    "def gradient_descent(Xs, Ys, gamma, W0, alpha, num_iters, monitor_freq):\n",
    "    W = W0\n",
    "    paramsHist = [W.copy()]\n",
    "    accuracyHist = [multinomial_logreg_error(Xs, Ys, W)]\n",
    "    for iter in range(num_iters):\n",
    "        W = W - alpha * multinomial_logreg_batch_grad(Xs, Ys, gamma, W, None)\n",
    "        if (iter + 1) % monitor_freq == 0:\n",
    "            accuracyHist.append(multinomial_logreg_error(Xs, Ys, W))\n",
    "            paramsHist.append(W.copy())\n",
    "    return accuracyHist, paramsHist\n",
    "\n",
    "\n",
    "# ALGORITHM 1: run stochastic gradient descent on a multinomial logistic regression objective, with regularization\n",
    "#\n",
    "# Xs              training examples (d * n)\n",
    "# Ys              training labels   (c * n)\n",
    "# gamma           L2 regularization constant\n",
    "# W0              the initial value of the parameters (c * d)\n",
    "# alpha           step size/learning rate\n",
    "# B               minibatch size\n",
    "# num_epochs      number of epochs (passes through the training set) to run\n",
    "# monitor_period  how frequently, in terms of batches (not epochs) to output the parameter vector\n",
    "#\n",
    "# returns         a list of model parameters vectors, one every \"monitor_period\" batches\n",
    "#                   to do this, you'll want code like the following:\n",
    "#                     models = []\n",
    "#                     models.append(W0.copy())   # (you may not need the copy if you don't mutate W0)\n",
    "#                     ...\n",
    "#                     for sgd_iteration in ... :\n",
    "#                       ...\n",
    "#                       # code to compute a single SGD update step here\n",
    "#                       ...\n",
    "#                       if (it % monitor_period == 0):\n",
    "#                         models.append(W)\n",
    "def sgd_minibatch(Xs, Ys, gamma, W0, alpha, B, num_epochs, monitor_period):\n",
    "    W = W0\n",
    "    paramsHist = [W.copy()]\n",
    "    accuracyHist = [multinomial_logreg_error(Xs, Ys, W)]\n",
    "    T = num_epochs * (Xs.shape[1] // B)\n",
    "    for iter in range(T):  # by pseudocode, T=epochs i guess\n",
    "        W = W - alpha * multinomial_logreg_batch_grad(\n",
    "            Xs, Ys, gamma, W, random.choice(Xs.shape[1], B)\n",
    "        )\n",
    "        if (iter + 1) % monitor_period == 0:\n",
    "            accuracyHist.append(multinomial_logreg_error(Xs, Ys, W))\n",
    "            paramsHist.append(W.copy())\n",
    "    return accuracyHist, paramsHist\n",
    "\n",
    "\n",
    "# ALGORITHM 2: run stochastic gradient descent with minibatching and sequential sampling order\n",
    "#\n",
    "# Xs              training examples (d * n)\n",
    "# Ys              training labels   (c * n)\n",
    "# gamma           L2 regularization constant\n",
    "# W0              the initial value of the parameters (c * d)\n",
    "# alpha           step size/learning rate\n",
    "# B               minibatch size\n",
    "# num_epochs      number of epochs (passes through the training set) to run\n",
    "# monitor_period  how frequently, in terms of batches (not epochs) to output the parameter vector\n",
    "#\n",
    "# returns         a list of model parameters vectors, one every \"monitor_period\" batches\n",
    "def sgd_minibatch_sequential_scan(\n",
    "    Xs, Ys, gamma, W0, alpha, B, num_epochs, monitor_period\n",
    "):\n",
    "    W = W0\n",
    "    paramsHist = [W.copy()]\n",
    "    accuracyHist = [multinomial_logreg_error(Xs, Ys, W)]\n",
    "    n = Xs.shape[1]\n",
    "    for epoch in range(num_epochs):  # by pseudocode, T=epochs i guess\n",
    "        for iter in range(n // B):\n",
    "            W = W - alpha * multinomial_logreg_batch_grad(\n",
    "                Xs, Ys, gamma, W, range(iter * B, (iter + 1) * B)\n",
    "            )\n",
    "            if ((epoch * (n // B)) + iter + 1) % monitor_period == 0:\n",
    "                accuracyHist.append(multinomial_logreg_error(Xs, Ys, W))\n",
    "                paramsHist.append(W.copy())\n",
    "    return accuracyHist, paramsHist\n",
    "\n",
    "\n",
    "# ALGORITHM 3: run stochastic gradient descent with minibatching and without-replacement sampling\n",
    "#\n",
    "# Xs              training examples (d * n)\n",
    "# Ys              training labels   (c * n)\n",
    "# gamma           L2 regularization constant\n",
    "# W0              the initial value of the parameters (c * d)\n",
    "# alpha           step size/learning rate\n",
    "# B               minibatch size\n",
    "# num_epochs      number of epochs (passes through the training set) to run\n",
    "# monitor_period  how frequently, in terms of batches (not epochs) to output the parameter vector\n",
    "#\n",
    "# returns         a list of model parameters vectors, one every \"monitor_period\" batches\n",
    "def sgd_minibatch_random_reshuffling(\n",
    "    Xs, Ys, gamma, W0, alpha, B, num_epochs, monitor_period\n",
    "):\n",
    "    W = W0\n",
    "    paramsHist = [W.copy()]\n",
    "    accuracyHist = [multinomial_logreg_error(Xs, Ys, W)]\n",
    "    n = Xs.shape[1]\n",
    "    shuffledIndices = numpy.arange(n)\n",
    "    for epoch in range(num_epochs):  # by pseudocode, T=epochs i guess\n",
    "        random.shuffle(shuffledIndices)\n",
    "        for iter in range(n // B):\n",
    "            W = W - alpha * multinomial_logreg_batch_grad(\n",
    "                Xs, Ys, gamma, W, shuffledIndices[iter * B : (iter + 1) * B]\n",
    "            )\n",
    "            if ((epoch * (n // B)) + iter + 1) % monitor_period == 0:\n",
    "                accuracyHist.append(multinomial_logreg_error(Xs, Ys, W))\n",
    "                paramsHist.append(W.copy())\n",
    "    return accuracyHist, paramsHist\n",
    "\n",
    "\n",
    "def timeit(f):\n",
    "    start = time()\n",
    "    val = f()\n",
    "    return val, time() - start\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    (Xs_tr, Ys_tr, Xs_te, Ys_te) = load_MNIST_dataset()\n",
    "\n",
    "    params = 2 * random.rand(Ys_tr.shape[0], Xs_tr.shape[0]) - 1\n",
    "    gd = lambda: gradient_descent(\n",
    "        Xs_tr,\n",
    "        Ys_tr,\n",
    "        0.0001,\n",
    "        2 * random.rand(Ys_tr.shape[0], Xs_tr.shape[0]) - 1,\n",
    "        1.0,\n",
    "        1000,\n",
    "        10,\n",
    "    )\n",
    "    (errors, paramHist), secs = timeit(gd)\n",
    "    for i, j in enumerate(errors):\n",
    "        print(f\"Accuracy Measurement {i+1}: {1-j}\")\n",
    "    print(f\"Gradient Descent (1000 Iters) Took {secs} Seconds\")\n",
    "\n",
    "    # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "    #               Three Algorithms\n",
    "    # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "    batch = 60\n",
    "    alpha = 0.05\n",
    "    monitor_period = 100\n",
    "    gamma = 0.0001\n",
    "    W0 = lambda: 2 * random.rand(Ys_tr.shape[0], Xs_tr.shape[0]) - 1\n",
    "    # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "    #                   Hyperparams\n",
    "    # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "    # SGD\n",
    "    sgd = lambda: sgd_minibatch(\n",
    "        Xs_tr,\n",
    "        Ys_tr,\n",
    "        gamma=gamma,\n",
    "        W0=W0(),\n",
    "        alpha=alpha,\n",
    "        B=batch,\n",
    "        num_epochs=10,\n",
    "        monitor_period=monitor_period,\n",
    "    )\n",
    "    (errorsSGD, paramHistSGD), secs = timeit(sgd)\n",
    "    for i, j in enumerate(errorsSGD):\n",
    "        print(f\"Accuracy Measurement {i+1}: {1-j}\")\n",
    "    print(f\"SGD Normal B={batch}, A={alpha}, gamma={gamma} Took {secs} Seconds\\n\")\n",
    "\n",
    "    # SGD Sequential\n",
    "    params = 2 * random.rand(Ys_tr.shape[0], Xs_tr.shape[0]) - 1\n",
    "    sgds = lambda: sgd_minibatch_sequential_scan(\n",
    "        Xs_tr,\n",
    "        Ys_tr,\n",
    "        gamma=gamma,\n",
    "        W0=W0(),\n",
    "        alpha=alpha,\n",
    "        B=batch,\n",
    "        num_epochs=10,\n",
    "        monitor_period=monitor_period,\n",
    "    )\n",
    "    (errorsSGDS, paramHistSGDS), secs = timeit(sgds)\n",
    "    for i, j in enumerate(errorsSGDS):\n",
    "        print(f\"Accuracy Measurement {i+1}: {1-j}\")\n",
    "    print(f\"SGD Sequential B={batch}, A={alpha}, gamma={gamma} Took {secs} Seconds\\n\")\n",
    "\n",
    "    # SGD Random Shuffling\n",
    "    params = 2 * random.rand(Ys_tr.shape[0], Xs_tr.shape[0]) - 1\n",
    "    sgdrs = lambda: sgd_minibatch_random_reshuffling(\n",
    "        Xs_tr,\n",
    "        Ys_tr,\n",
    "        gamma=gamma,\n",
    "        W0=W0(),\n",
    "        alpha=alpha,\n",
    "        B=batch,\n",
    "        num_epochs=10,\n",
    "        monitor_period=monitor_period,\n",
    "    )\n",
    "    (errorsSGDRS, paramHist), secs = timeit(sgdrs)\n",
    "    for i, j in enumerate(errorsSGDRS):\n",
    "        print(f\"Accuracy Measurement {i+1}: {1-j}\")\n",
    "    print(\n",
    "        f\"SGD Random Shuffling B={batch}, A={alpha}, gamma={gamma} Took {secs} Seconds\\n\"\n",
    "    )\n",
    "    # GRAPHING\n",
    "    # gradient descent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multinomial_logreg_batch_loss(Xs, Ys, gamma, W, ii=None):\n",
    "    if ii is None:\n",
    "        ii = range(Xs.shape[1])\n",
    "        X = Xs\n",
    "        Y = Ys\n",
    "    else:\n",
    "        X = Xs[:, ii]\n",
    "        Y = Ys[:, ii]\n",
    "\n",
    "    (d, n) = Xs.shape\n",
    "    frobNorm = numpy.linalg.norm(W) ** 2\n",
    "    return (\n",
    "        numpy.sum(Y * (-numpy.log(softmax(W @ X, axis=0)))) / len(ii)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "[nan nan nan  0.  0. nan nan -0.  0.  0.]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/vp/4lp970hs40g8l460v3lfvvmc0000gn/T/ipykernel_24455/1417842691.py:5: RuntimeWarning: divide by zero encountered in log\n",
      "  a = Y * (-numpy.log(softmax(W @ X, axis=0)))\n",
      "/var/folders/vp/4lp970hs40g8l460v3lfvvmc0000gn/T/ipykernel_24455/1417842691.py:5: RuntimeWarning: invalid value encountered in multiply\n",
      "  a = Y * (-numpy.log(softmax(W @ X, axis=0)))\n"
     ]
    }
   ],
   "source": [
    "X = Xs_te\n",
    "Y = Ys_te\n",
    "W = paramHist[-1]\n",
    "\n",
    "a = Y * (-numpy.log(softmax(W @ X, axis=0)))\n",
    "print(Y[:, 0])\n",
    "print(a[:, 0])\n",
    "print('\\n\\n\\n\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/vp/4lp970hs40g8l460v3lfvvmc0000gn/T/ipykernel_24455/2111326917.py:13: RuntimeWarning: divide by zero encountered in log\n",
      "  numpy.sum(Y * (-numpy.log(softmax(W @ X, axis=0)))) / len(ii)\n",
      "/var/folders/vp/4lp970hs40g8l460v3lfvvmc0000gn/T/ipykernel_24455/2111326917.py:13: RuntimeWarning: invalid value encountered in multiply\n",
      "  numpy.sum(Y * (-numpy.log(softmax(W @ X, axis=0)))) / len(ii)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "nan"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multinomial_logreg_batch_loss(Xs_te, Ys_te, 0.0001, paramHist[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/vp/4lp970hs40g8l460v3lfvvmc0000gn/T/ipykernel_24455/2111326917.py:13: RuntimeWarning: divide by zero encountered in log\n",
      "  numpy.sum(Y * (-numpy.log(softmax(W @ X, axis=0)))) / len(ii)\n",
      "/var/folders/vp/4lp970hs40g8l460v3lfvvmc0000gn/T/ipykernel_24455/2111326917.py:13: RuntimeWarning: invalid value encountered in multiply\n",
      "  numpy.sum(Y * (-numpy.log(softmax(W @ X, axis=0)))) / len(ii)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 4 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# GRAPHING\n",
    "# gradient descent\n",
    "trLossGD = [\n",
    "    multinomial_logreg_batch_loss(Xs_tr, Ys_tr, 0.0001, weight)\n",
    "    for weight in paramHist\n",
    "]\n",
    "trErrorGD = [multinomial_logreg_error(Xs_tr, Ys_tr, weight) for weight in paramHist]\n",
    "teLossGD = [\n",
    "    multinomial_logreg_batch_loss(Xs_te, Ys_te, 0.0001, weight)\n",
    "    for weight in paramHist\n",
    "]\n",
    "teErrorGD = [multinomial_logreg_error(Xs_te, Ys_te, weight) for weight in paramHist]\n",
    "iters = [(10 * i + 1) for i in range(len(paramHist))]\n",
    "\n",
    "fig, axs = pyplot.subplots(2, 2)\n",
    "axs[0, 0].plot(iters, trLossGD)\n",
    "axs[0, 0].set_title(\"Training Loss\")\n",
    "axs[0, 1].plot(iters, trErrorGD, \"tab:orange\")\n",
    "axs[0, 1].set_title(\"Training Error\")\n",
    "axs[1, 0].plot(iters, teLossGD, \"tab:green\")\n",
    "axs[1, 0].set_title(\"Test Loss\")\n",
    "axs[1, 1].plot(iters, teErrorGD, \"tab:red\")\n",
    "axs[1, 1].set_title(\"Test Error\")\n",
    "\n",
    "for ax in axs.flat:\n",
    "    ax.set(xlabel=\"Iteration\", ylabel=\"Loss / Error\")\n",
    "\n",
    "# Hide x labels and tick labels for top plots and y ticks for right plots.\n",
    "for ax in axs.flat:\n",
    "    ax.label_outer()\n",
    "pyplot.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
